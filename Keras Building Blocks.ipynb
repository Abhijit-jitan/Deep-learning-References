{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Losses-:\" data-toc-modified-id=\"Losses-:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Losses :</a></span><ul class=\"toc-item\"><li><span><a href=\"#A)-Probabilistic-losses-:\" data-toc-modified-id=\"A)-Probabilistic-losses-:-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>A) Probabilistic losses :</a></span></li><li><span><a href=\"#B)-Regression-losses-:\" data-toc-modified-id=\"B)-Regression-losses-:-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>B) Regression losses :</a></span></li><li><span><a href=\"#C)-Hinge-losses-:\" data-toc-modified-id=\"C)-Hinge-losses-:-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>C) Hinge losses :</a></span></li></ul></li><li><span><a href=\"#Metrics-:\" data-toc-modified-id=\"Metrics-:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Metrics :</a></span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy-metrics-:\" data-toc-modified-id=\"Accuracy-metrics-:-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Accuracy metrics :</a></span></li><li><span><a href=\"#Probabilistic-metrics-:\" data-toc-modified-id=\"Probabilistic-metrics-:-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Probabilistic metrics :</a></span></li><li><span><a href=\"#Regression-metrics-:\" data-toc-modified-id=\"Regression-metrics-:-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Regression metrics :</a></span></li><li><span><a href=\"#Classification-metrics-based-on-True/False-positives-&amp;-negatives\" data-toc-modified-id=\"Classification-metrics-based-on-True/False-positives-&amp;-negatives-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Classification metrics based on True/False positives &amp; negatives</a></span></li><li><span><a href=\"#Image-segmentation-metrics-:\" data-toc-modified-id=\"Image-segmentation-metrics-:-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Image segmentation metrics :</a></span></li><li><span><a href=\"#Hinge-metrics-for-&quot;maximum-margin&quot;-classification\" data-toc-modified-id=\"Hinge-metrics-for-&quot;maximum-margin&quot;-classification-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Hinge metrics for \"maximum-margin\" classification</a></span></li></ul></li><li><span><a href=\"#Optimizers-:\" data-toc-modified-id=\"Optimizers-:-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Optimizers :</a></span></li><li><span><a href=\"#Activation-Functions-:\" data-toc-modified-id=\"Activation-Functions-:-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Activation Functions :</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A.1) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "---------- A.1) Model Class ----------\n",
    "## Model Class: Groups layers into object with training & inference features\n",
    "@ tf.keras.Model()\n",
    " * inputs:input(s) of the model(keras.Input object or list of keras.Input objects)\n",
    " * outputs:output(s) of model\n",
    " * name:(str) name of the model\n",
    "    \n",
    "# Methods:\n",
    "\n",
    "summary(): Prints summary of network\n",
    "@ Model.summary(line_length=None,positions=None,print_fn=None)\n",
    " * line_length: Total length of printed lines \n",
    " * positions: Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.].\n",
    " * print_fn: Print function to use. Defaults to print. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary.\n",
    ">>>\n",
    "\n",
    "get_layer(): Retrieves layer based on either its name or index.\n",
    "@ Model.get_layer(name=None,index=None)\n",
    " * name:(str) name of layer\n",
    " * index:(int) index of layer    \n",
    ">>> \n",
    "    \n",
    "## https://keras.io/api/models/model/\n",
    "\n",
    "\n",
    "## A.2) Sequential\n",
    "\n",
    "\n",
    "\n",
    "---------- A.2) Sequential ----------\n",
    "\n",
    "## Sequential(): Groups linear stack of layers into 'Model()' &  provides training & inference features to \n",
    "@ Sequential(layers=None,name=None)\n",
    " * layers=Layer instance to add\n",
    " * name=(str)Name of the specific layer \n",
    "\n",
    ">>> model=Sequential()\n",
    "\n",
    "# methods:\n",
    "add(): Adds a layer instance on top of layer stack\n",
    "@ Sequential.add(layer) \n",
    " * layer: layer instance\n",
    ">>> model.add()\n",
    "\n",
    "pop(): Removes last layer in model\n",
    "@ Sequential.pop() \n",
    ">>> modeldel.add()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A.3) Compile Training \n",
    "\n",
    "\n",
    "---------- A.3) Model Compile() ----------\n",
    "## Compile(): Configures model for training\n",
    "@ Model.compile(optimizer=\"rmsprop\",loss=None,metrics=None,loss_weights=None,weighted_metrics=None,run_eagerly=None,steps_per_execution=None)\n",
    "    \n",
    " * optimizer:(str) name of optimizer to use\n",
    " * loss:(str) name of loss Function to be used \n",
    " * metrics:(list) list of name of metrics to be evaluated by model during training & testing\n",
    " ** loss_weights:(list|dict) specifying weight loss contributions of different model outputs\n",
    " * weighted_metrics:(list) list of metrics to be evaluated & weighted by 'sample_weight' or 'class_weight' during training & testing\n",
    " * run_eagerly:(bool)If True:models logic will not be wrapped in 'tf.function'\n",
    " * steps_per_execution:(int)no. of batches to run during each 'tf.function call'\n",
    "    Running multiple batches inside 'single tf.function call' can greatly improve performance on TPUs or small models with a large Python overhead.\n",
    "    \n",
    "\n",
    "# Property:\n",
    "run_eagerly : Settable attribute indicating whether model should run eagerly\n",
    "- model will be run step by step (slower but better for debugging).It returns (bool)whether model should run eagerly\n",
    "@ tf.keras.Model.run_eagerly\n",
    ">>> model.run_eagerly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A.4) Model Training \n",
    "\n",
    "---------- A.4.a) Model Training  ----------\n",
    "## fit(): Trains model for fixed number of epochs (iterations on dataset)\n",
    "@ Model.fit(x=None,y=None,batch_size=None,epochs=1,verbose=1,callbacks=None,validation_split=0.0,validation_data=None,\n",
    "          shuffle=True,class_weight=None,sample_weight=None,initial_epoch=0,steps_per_epoch=None,validation_steps=None,\n",
    "          validation_batch_size=None,validation_freq=1,max_queue_size=10,workers=1,use_multiprocessing=False)\n",
    " * x:Input data\n",
    "     - numpy array | list of arrays | Tf tensor | list of tensors | dict mapping input names corresponding array|tensors\n",
    "     - tf.data(dataset)return tuple of either(inputs,targets) or (inputs,targets,sample_weights)\n",
    "     - generator | keras.utils.Sequence returning(inputs,targets) or (inputs,targets,sample_weights)\n",
    " * y: Target data (it could be either Numpy array(s) | TensorFlow tensor(s))\n",
    " * batch_size:(int or None) No. of samples per gradient update\n",
    " * epochs:(int) no. of times to train model with whole dataset\n",
    " * verbose=0: silent | 1: progress bar | 2: one line per epoch\n",
    " * callbacks:(list of keras.callbacks.Callback instances) List of callbacks to apply during training\n",
    " * validation_split:(float 0-1)Fraction of training data to be used as validation data\n",
    " * validation_data:Data to evaluate loss & any model metrics at end of each epoch\n",
    " * shuffle:(bool)whether to shuffle training data before each epoch(ignored when x is generator)\n",
    " ** class_weight:dict. mapping class indices(int) to weight(float) value, used for weighting loss function (during training only)\n",
    " ** sample_weight:numpy array of weights for training samples,used for weighting loss function(during training only)\n",
    " * initial_epoch:(int)Epoch at which to start training(useful for resuming a previous training run)\n",
    " * steps_per_epoch:(int | None)Total number of steps(batches of samples) before declaring one epoch finished & starting next epoch\n",
    " * validation_steps:Total no. of steps(batches of samples) to draw before stopping when performing validation at end of every epoch(if 'validation_data' provided & is tf.data dataset)\n",
    " * validation_batch_size:(int | None)No. of samples per validation batch(default to batch_size)\n",
    " * validation_freq=If int: specifies how many training epochs to run before new validation run is performed (e.g: validation_freq=2 runs validation every 2 epochs)\n",
    "                  =If Container: specifies epochs on which to run validation(e.g. validation_freq=[1,2,10] runs validation at end of 1st,2nd & 10th epochs)\n",
    " * max_queue_size:(int) Max size for generator queue(Used for generator | keras.utils.Sequence input only)\n",
    " * workers:(int)Max no. of processes to spin up when using process-based threading (Used for generator | keras.utils.Sequence input only) \n",
    " * use_multiprocessing:(bool) If True:use process-based threading (Used for generator | keras.utils.Sequence input only) \n",
    "     Note not pass 'non-picklable' arguments to generator as they cant be passed easily to children processes\n",
    "\n",
    ">>> history=model.fit(x=train,steps_per_epoch=len(train),epochs=15,verbose=2,callbacks=[callback,rl_scheduler])\n",
    " \n",
    "# Returns:(History object)Record of training loss values & metrics values at successive epochs,validation loss values & validation metrics values\n",
    "\n",
    "   \n",
    "                            ---------- A.4.b) train_on_batch ----------    \n",
    "## train_on_batch() : Runs single gradient update on single batch of data\n",
    "@ Model.train_on_batch(x,y=None,sample_weight=None,class_weight=None,reset_metrics=True,return_dict=False)\n",
    " * x:Input data\n",
    "     - numpy array | list of arrays | Tf tensor | list of tensors | dict mapping input names corresponding array|tensors\n",
    "     - tf.data(dataset)return tuple of either(inputs,targets) or (inputs,targets,sample_weights)\n",
    "     - generator | keras.utils.Sequence returning(inputs,targets) or (inputs,targets,sample_weights)\n",
    " * y: Target data (it could be either Numpy array(s) | TensorFlow tensor(s))\n",
    " ** class_weight:dict. mapping class indices(int) to weight(float) value,used for weighting loss function (during training only)\n",
    " ** sample_weight:numpy array of weights for training samples,used for weighting loss function(during training only)\n",
    " * reset_metrics: If True:metrics returned will be only for this batch\n",
    "                  If False: metrics will be statefully accumulated across batches\n",
    " * return_dict: If True:loss & metric results are returned as dict, with each key being name of metric. \n",
    "                If False: they are returned as list\n",
    ">>>                     \n",
    "# Returns: Scalar training loss(if model has single output & no metrics) | list of scalars(if model has multiple outputs &| metrics)\n",
    "          attribute 'model.metrics_names' will give you display labels for scalar output    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A.5) Model Prediction : \n",
    "\n",
    "---------- A.5.a) predict() ----------\n",
    "## predict method : Generates output predictions for input samples\n",
    "@ Model.predict(x,batch_size=None,verbose=0,steps=None,callbacks=None,max_queue_size=10,workers=1,use_multiprocessing=False)\n",
    " * x:Input data\n",
    "     - numpy array | list of arrays | Tf tensor | list of tensors | dict mapping input names corresponding array|tensors\n",
    "     - tf.data(dataset)return tuple of either(inputs,targets) or (inputs,targets,sample_weights)\n",
    "     - generator | keras.utils.Sequence returning(inputs,targets) or (inputs,targets,sample_weights)\n",
    " * batch_size:(int or None) No. of samples per gradient update\n",
    " * verbose=0: silent | 1: progress bar | 2: one line per epoch\n",
    " * callbacks:(list of keras.callbacks.Callback instances) List of callbacks to apply during training\n",
    " * max_queue_size:(int) Max size for generator queue(Used for generator | keras.utils.Sequence input only)\n",
    " * workers:(int)Max no. of processes to spin up when using process-based threading (Used for generator | keras.utils.Sequence input only) \n",
    " * use_multiprocessing:(bool) If True:use process-based threading (Used for generator | keras.utils.Sequence input only) \n",
    "     Note not pass 'non-picklable' arguments to generator as they cant be passed easily to children processes\n",
    ">>> \n",
    "\n",
    "\n",
    "                         ---------- A.5.b) predict_on_batch() ----------\n",
    "## predict_on_batch method: Returns predictions for single batch of samples\n",
    "@ Model.predict_on_batch(x)\n",
    " * x: Input data\n",
    "        - If numpy array | list of arrays (in case the model has multiple inputs)\n",
    "        - If tensor | list of tensors (in case the model has multiple inputs)\n",
    ">>>\n",
    "# Returns: numpy array(s) of predictions\n",
    "\n",
    "                          ---------- A.5.c) test_on_batch() ----------\n",
    "## test_on_batch() : Test model on single batch of samples\n",
    "@ Model.test_on_batch(x,y=None,sample_weight=None,reset_metrics=True,return_dict=False)\n",
    " * x:Input data\n",
    "     - numpy array | list of arrays | Tf tensor | list of tensors | dict mapping input names corresponding array|tensors\n",
    "     - tf.data(dataset)return tuple of either(inputs,targets) or (inputs,targets,sample_weights)\n",
    "     - generator | keras.utils.Sequence returning(inputs,targets) or (inputs,targets,sample_weights)\n",
    " * y: Target data (it could be either Numpy array(s) | TensorFlow tensor(s))\n",
    " ** sample_weight:numpy array of weights for training samples,used for weighting loss function(during training only)\n",
    " * reset_metrics: If True:metrics returned will be only for this batch\n",
    "                  If False: metrics will be statefully accumulated across batches\n",
    " * return_dict: If True:loss & metric results are returned as dict, with each key being name of metric. \n",
    "                If False: they are returned as list\n",
    ">>>                     \n",
    "# Returns: loss (if model has single output & no metrics) | list of scalars (if model has multiple outputs &| metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A.6) Model Evaluate \n",
    "\n",
    "---------- A.6) Model Evaluate ---------\n",
    "## evaluate method : Returns loss value & metrics values for model in test mode\n",
    "@ Model.evaluate(x=None,y=None,batch_size=None,verbose=1,sample_weight=None,steps=None,callbacks=None,\n",
    "               max_queue_size=10,workers=1,use_multiprocessing=False,return_dict=False)\n",
    " * x:Input data\n",
    "     - numpy array | list of arrays | Tf tensor | list of tensors | dict mapping input names corresponding array|tensors\n",
    "     - tf.data(dataset)return tuple of either(inputs,targets) or (inputs,targets,sample_weights)\n",
    "     - generator | keras.utils.Sequence returning(inputs,targets) or (inputs,targets,sample_weights)\n",
    "        \n",
    " * y: Target data (it could be either Numpy array(s) | TensorFlow tensor(s))\n",
    " * batch_size:(int or None) No. of samples per gradient update\n",
    " * steps:(int or None)Total no. of steps(batches of samples) before declaring evaluation round finished\n",
    " * verbose=0: silent | 1: progress bar | 2: one line per epoch\n",
    " ** sample_weight:numpy array of weights for training samples,used for weighting loss function(during training only)\n",
    " * callbacks:(list of keras.callbacks.Callback instances) List of callbacks to apply during training\n",
    " * max_queue_size:(int) Max size for generator queue(Used for generator | keras.utils.Sequence input only)\n",
    " * workers:(int)Max no. of processes to spin up when using process-based threading (Used for generator | keras.utils.Sequence input only) \n",
    " * use_multiprocessing:(bool) If True:use process-based threading (Used for generator | keras.utils.Sequence input only) \n",
    "     Note not pass 'non-picklable' arguments to generator as they cant be passed easily to children processes\n",
    " * return_dict: If True:loss & metric results are returned as dict, with each key being name of metric. \n",
    "                If False: they are returned as list\n",
    ">>>                     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A.7) Saving & Load (serialization)\n",
    "- Can be saved during & after training(can resume where it left off & avoid long training times)\n",
    "- Can be shared & others can recreate work(code to create model,trained weights,parameters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---------- A.7.a) Save Model ----------\n",
    "## i) save(): Saves model to 'Tensorflow SavedModel' or 'single HDF5 file'\n",
    "saves : Architecture,Weights,Optimizer & Optimizer_State(resume training exactly where you left off),Learning-rate,Loss \n",
    "@ Model.save(filepath,overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)\n",
    " * filepath:(str)PathLike,path to SavedModel or H5 file to save model\n",
    " * overwrite:Whether to silently overwrite any existing file at target location | provide user with manual prompt\n",
    " * include_optimizer=True:save optimizers state together\n",
    " * save_format='tf':whether to save model to Tensorflow SavedModel\n",
    "              ='h5':whether to save model to HDF5\n",
    " * signatures: Signatures to save with SavedModel(Applicable to the 'tf' format only)\n",
    " ** options: tf.saved_model.SaveOptions object that specifies options for saving to SavedModel\n",
    ">>> model.save('model.h5')   \n",
    "\n",
    "## ii) save_model(): Saves model as 'TensorFlow SavedModel' or 'HDF5 file'\n",
    "saves : Architecture,Weights,Optimizer & Optimizer_State(resume training exactly where you left off),Learning-rate,Loss \n",
    "@ models.save_model(model,filepath,overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)\n",
    " * model: Keras model to be saved\n",
    " * filepath=(string|pathlib.Path()|h5py.File()): path where to save model\n",
    " * overwrite:Whether to silently overwrite any existing file at target location | provide user with manual prompt\n",
    " * include_optimizer=True:save optimizers state together\n",
    " * save_format='tf':whether to save model to Tensorflow SavedModel\n",
    "              ='h5':whether to save model to HDF5\n",
    " * signatures: Signatures to save with SavedModel(Applicable to the 'tf' format only)\n",
    " ** options: 'tf.saved_model.SaveOptions()', that specifies options for saving to SavedModel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                        ---------- A.7.b) Load & Clone Model ----------\n",
    "## i) Load_model:\n",
    "@ models.load_model(filepath,custom_objects=None,compile=True,options=None)\n",
    " * filepath=(string|pathlib.Path()|h5py.File()): path of saveed model\n",
    " ** custom_objects: dict. mapping names(str) to custom classes | functions to be considered during deserialization.\n",
    " * compile:(bool)whether to compile model after loading\n",
    " ** options: 'tf.saved_model.LoadOptions()',that specifies options for Loading from SavedModel\n",
    "\n",
    ">>> from tensorflow.keras.models import load_model\n",
    ">>> new_model=load_model('model.h5')\n",
    "\n",
    "## ii) clone_model(): Clone any Model,creates new layers(and thus new weights) instead of sharing weights of existing layers \n",
    "@ tf.keras.models.clone_model(model,input_tensors=None,clone_function=None)\n",
    " * model: Instance of Model (could be a functional model or Sequential model).\n",
    " ** input_tensors:(list of input tensors|InputLayer()) build model upon\n",
    "       If not provided, placeholders will be created\n",
    " * clone_function: It takes as argument layer instance to be cloned & returns corresponding layer instance to be used in model copy       \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---------- A.7.c) Weights(Save & Load) ---------- \n",
    "## i) save_weights():Saves all layer weights(saves in HDF5|Tf)\n",
    "new_model must same architecture as old_model before weights can be saved\n",
    "@ Model.save_weights(filepath,overwrite=True,save_format=None,options=None)\n",
    " * filepath: (str|PathLike)path of file to save weights\n",
    " * overwrite:Whether to silently overwrite any existing file at target location | provide user with manual prompt\n",
    " * save_format='tf':whether to save model to Tensorflow SavedModel\n",
    "              ='h5':whether to save model to HDF5\n",
    " ** options: tf.saved_model.SaveOptions object that specifies options for saving to SavedModel\n",
    ">>> model.load_weights('model_weights.h5')\n",
    "\n",
    "## ii) set_weights(): Sets weights of the layer, from Numpy arrays\n",
    "@ Model.set_weights(weights)\n",
    " * weights:(list of numpy arrays)no of arrays & their shape must match no. of dimensions of weights of layer(it should match o/p of 'get_weights')\n",
    ">>> model.save_weights('model_weights.h5')\n",
    "\n",
    "## iii) get_weights(): Retrieves weights of the model(Returns flat 'list of numpy' arrays)\n",
    "@ Model.get_weights()\n",
    "\n",
    "## iv) load_weights(): Loads all layer weights,either from TensorFlow | HDF5 weight file\n",
    "@ Model.load_weights(filepath,by_name=False,skip_mismatch=False,options=None)\n",
    "filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights).\n",
    " * by_name:(bool)whether to load weights by name|by topological order(only topological loading is supported for weight files in TensorFlow format)\n",
    " * skip_mismatch:(bool)whether to skip loading of layers where there is mismatch in 'no. of weights' or mismatch in 'shape of weight'(only valid when by_name=True)\n",
    " ** options:'tf.train.CheckpointOptions()',that specifies options for loading weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---------- A.7.d) Model Configuration ----------\n",
    "## i) get_config(): Returns 'python dict.(serializable)' config of layer \n",
    "@ Model.get_config()\n",
    "\n",
    "## ii) from_config():Creates a layer from its config(reverse of 'get_config')\n",
    "@ Model.from_config(config,custom_objects=None)\n",
    " * config:Python dict. ,typically output of 'get_config'\n",
    "\n",
    "## iii) model_from_config(): Instantiates Keras model from its config\n",
    "   - Keras model instance (uncompiled)\n",
    "@ tf.keras.models.model_from_config(config,custom_objects=None)\n",
    " * config: Configuration dict.\n",
    " ** custom_objects:(dict.)mapping names(str) to custom classes or functions to be considered during deserialization\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "---------- A.7.e) Model Configuration('json') ----------\n",
    "## i) model.to_json(): Returns 'JSON string' containing network configuration\n",
    "@ Model.to_json(**kwargs)\n",
    " **kwargs: Additional keyword arguments to be passed to 'json.dumps()'\n",
    ">>> model=model.to_json()\n",
    "\n",
    "## ii) model_from_json(): Parses JSON model configuration string & returns model instance\n",
    " @ tf.keras.models.model_from_json(json_string,custom_objects=None)\n",
    " * json_string:'JSON string' encoding model configuration\n",
    " ** custom_objects:dict. mapping names(strings) to custom classes | functions to be considered during deserialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base Layer class\n",
    "- All layers inherited from \"Layer Class\"\n",
    "\n",
    "https://keras.io/api/layers/base_layer/\n",
    "\n",
    "\n",
    "import tensorflow.keras.layers\n",
    "@ tf.keras.layers.Layer(trainable=True,name=None,dtype=None,dynamic=False)\n",
    "  * trainable:(Bool)whether layers variables should be trainable\n",
    "  * name:(str) Name of the Layer\n",
    "  * dtype: dtype for layer 'computations' & 'weights'\n",
    "  * dynamic:True - layer only run eagerly & should not be used to generate static computation graph\n",
    "           :False- layer safely used to generate static computation graph\n",
    "\n",
    "## Attributes :\n",
    "* name      # name of layer\n",
    "* dtype      # dtype of layer 'computations' & 'weights'\n",
    "* trainable_weights      # List of variables to be included in backprop(trainable parameters)\n",
    "* non_trainable_weights      # List of variables that should not be included in backprop(non-trainable parameters)\n",
    "* weights      # Concatenation of lists trainable_weights & non_trainable_weights\n",
    "* trainable      # Whether the layer should be trained\n",
    "* input_spec      #  'InputSpec()'  specifying constraints on inputs that can be accepted by layer\n",
    "\n",
    "## property:\n",
    "* model.weights|conv1.weights      # list of all weights(by layers)\n",
    "* model.trainable_weights|conv1.trainable_weights      # list of trainable weights(by layers)\n",
    "* model.non_trainable_weights|conv1.non_trainable_weights      # list of non-trainable weights(by layers)\n",
    "* model.trainable|conv1.trainable      # (Bool)whether layers variables should be trainable\n",
    "\n",
    "## Methods:\n",
    "* model.get_weights()|layer.get_weights()      # Return current layer weights\n",
    "* model.set_weights()|layer.set_weights()      # set weights of layer(from nd-array)\n",
    "* model.get_config()|layer.get_config()        # Return configuration of layer\n",
    "* model.add_loss(losses)|layer.add_loss(losses)      # Add Loss tensor\n",
    "* model.add_metric(value,name)|layer.add_metric(value,name)      # Add Loss tensor\n",
    "* model.losses|layer.losses        # Return losses(tensor) added by 'add_loss()' \n",
    "* model.metrics|layer.metrics      # List of metrics added using 'add_metric()'\n",
    "* model.dynamic|layer.dynamic      # Whether layer is dynamic (eager-only)\n",
    "\n",
    "\n",
    "# Preprocessing Layer :\n",
    "**Core preprocessing**\n",
    "- TextVectorization layer\n",
    "- Normalization layer\n",
    "\n",
    "**Categorical data preprocessing**\n",
    "- CategoryEncoding layer\n",
    "- Hashing layer\n",
    "- Discretization layer\n",
    "- StringLookup layer\n",
    "- IntegerLookup layer\n",
    "- CategoryCrossing layer\n",
    "\n",
    "**Image preprocessing & augmentation**\n",
    "- Resizing layer\n",
    "- Rescaling layer\n",
    "- CenterCrop layer\n",
    "- RandomCrop layer\n",
    "- RandomFlip layer\n",
    "- RandomTranslation layer\n",
    "- RandomRotation layer\n",
    "- RandomZoom layer\n",
    "- RandomHeight layer\n",
    "- RandomWidth layer\n",
    "\n",
    "\n",
    "## Core preprocessing layers\n",
    "**i) TextVectorization :** Text Vectorization Layer\n",
    "\n",
    "https://keras.io/api/layers/preprocessing_layers/core_preprocessing_layers/text_vectorization/\n",
    "\n",
    "\n",
    "**ii) Normalization :** Feature-wise normalization of the data\n",
    "\n",
    "https://keras.io/api/layers/preprocessing_layers/core_preprocessing_layers/normalization/\n",
    "\n",
    "a) TextVectorization(): Transforms 'batch of strings'(one sample=one string) into 'list of token indices'(one sample=1D tensor of int token indices)\n",
    "   or 'dense representation'(one sample=1D tensor of float values representing data about sample tokens)\n",
    " - By calling,'adapt()' to the layer on dataset: it analyze dataset & 'count frequency' of individual string & create 'vocabulary' out of them\n",
    " - vocabulary can 'unlimited size'|'capped',\n",
    "     if more unique values in i/p than max vocabulary size: most frequent terms will be used to create vocabulary\n",
    "\n",
    "* TextVectorization steps:\n",
    "   i) standardize each sample(lowercasing+punctuation stripping) \n",
    "   ii)split each sample into substrings(words) \n",
    "   iii) Recombine substrings into tokens (usually ngrams)\n",
    "   iv) index tokens(associate unique int value with each token) \n",
    "    v) transform each sample using this index,either 'int vector' | 'dense float vector'\n",
    "\n",
    "@ TextVectorization(max_tokens=None,standardize=\"lower_and_strip_punctuation\",split=\"whitespace\",ngrams=None,output_mode=\"int\",output_sequence_length=None,pad_to_max_tokens=True,**kwargs)\n",
    " * max_tokens:max size of vocabulary for this layer.\n",
    "     If None:there is no cap on size of vocabulary.\n",
    "    Note:vocabulary contains 1 OOV token,so effective no. of tokens is(max_tokens-1-(1 if output==\"int\" else 0))\n",
    " ** standardize:specification for standardization to apply to input text\n",
    "       None:no standardization\n",
    "      'lower_and_strip_punctuation':lowercase & remove punctuation or Callable\n",
    " ** split: specification for splitting input text. \n",
    "      None:no splitting\n",
    "      'whitespace':split on ASCII whitespace or a Callable\n",
    " ** ngrams:specification for 'ngrams' to create from possibly-split input text\n",
    "       None:no ngrams will be created\n",
    "       (int|tuple of int):passing an int will create ngrams up to that int & \n",
    "        passing tuple of int will create ngrams for specified values in tuple\n",
    " ** output_mode: output of layer {\"int\"|\"binary\"|\"count\"|\"tf-idf\"}\n",
    "    -\"int\":Returns int indices,one int index per split string token.\n",
    "        output==\"int\":0 reserved for masked locations(reduces 'vocab size to max_tokens-2' instead of 'max_tokens-1')\n",
    "    \"binary\":Returns 'int array' per batch,of 'vocab_size'|'max_tokens size',containing '1s' in all elements where token mapped to that index exists at least once in batch item\n",
    "    \"count\":Returns as \"binary\" but int array contains count of frequency that index appeared in batch item \n",
    "    \"tf-idf\": Returns as \"binary\",but TF-IDF algorithm is applied to find value in each token slot\n",
    " * output_sequence_length: (Only valid in int mode)\n",
    "    If set: o/p ahave,its time dimension padded|truncated to exactly output_sequence_length values,resulting 'tensor of shape[batch_size,output_sequence_length]' regardless of how many tokens resulted from splitting step \n",
    " * pad_to_max_tokens:Only valid in \"binary\", \"count\", and \"tf-idf\" modes.\n",
    "    If True: o/p will have its feature axis padded to 'max_tokens' even if number of unique tokens in vocabulary is less than max_tokens\n",
    "        resulting in 'tensor of shape[batch_size,max_tokens]' regardless of vocabulary size\n",
    "\n",
    ">>> vectorize_layer=TextVectorization(max_tokens=5000,output_mode='int',output_sequence_length=4)\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "b) Normalization(): Feature-wise normalization of data\n",
    "- Normalizes inputs into distribution centered around 0 with std 1\n",
    "'adapt':Compute mean & variance of data,store them layer weights.\n",
    "        adapt should be called before fit() , evaluate() , predict()\n",
    "\n",
    "@ Normalization(axis=-1,mean=None,variance=None)\n",
    " * axis:(int|tuple of int)axis that should be \"kept\"\n",
    " * mean:'mean value' to use during normalization(value broadcasted to shape of kept axes above)\n",
    " * variance:'variance value' to use during normalization(value broadcasted to shape of kept axes above) \n",
    "\n",
    ">>> layer=Normalization(mean=3.,variance=2.)\n",
    ">>> layer(input_data)\n",
    "\n",
    "## Categorical data preprocessing layers\n",
    "\n",
    "**i) CategoryEncoding:** Provides options for condensing data into 'categorical encoding'\n",
    "\n",
    "https://keras.io/api/layers/preprocessing_layers/categorical/category_encoding/\n",
    "    \n",
    "\n",
    "**ii) Hashing:** Implements categorical feature hashing, also known as \"hashing trick\"\n",
    "    \n",
    "https://keras.io/api/layers/preprocessing_layers/categorical/hashing/ \n",
    "    \n",
    "    \n",
    "**iii) Discretization :** Buckets data into discrete ranges\n",
    "\n",
    "https://keras.io/api/layers/preprocessing_layers/categorical/discretization/\n",
    "    \n",
    "    \n",
    "**iv) StringLookup :** Maps strings from a vocabulary to integer indices\n",
    "\n",
    "https://keras.io/api/layers/preprocessing_layers/categorical/string_lookup/\n",
    "    \n",
    "    \n",
    "**v) IntegerLookup :** Maps integers from a vocabulary to integer indices\n",
    "\n",
    "https://keras.io/api/layers/preprocessing_layers/categorical/integer_lookup/\n",
    "    \n",
    "\n",
    "**vi) CategoryCrossing:** Concatenates 'multiple categorical inputs' into 'single categorical output'\n",
    "\n",
    "https://keras.io/api/layers/preprocessing_layers/categorical/category_crossing/ \n",
    "\n",
    "i) CategoryEncoding: Provides options for condensing data into 'categorical encoding'\n",
    "- Accepts integer as inputs & outputs dense representation (one sample=1-index tensor of float values representing data about the sample's tokens) of those inputs\n",
    "@ tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=None,output_mode=\"binary\",sparse=False)\n",
    " * max_tokens: maximum size of vocabulary for this layer.\n",
    " * output_mode: Specification for output of layer {\"binary\"|\"count\"|\"tf-idf\"}\n",
    "   \"binary\": Outputs single int array per batch of either 'vocab_size'|'max_tokens size' containing 1s in all elements where token mapped to that index exists at least once in the batch item.\n",
    "   \"count\": Outputs int array contains count of number of times token at that index appeared in batch item.\n",
    "   \"tf-idf\": TF-IDF algorithm is applied to find value in each token slot.\n",
    " * sparse:(Bool) If True:returns SparseTensor instead of a dense Tensor\n",
    "                                                            \n",
    "Multi-hot encoding data: if you know in advance number of tokens\n",
    ">>> layer=tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=4,output_mode=\"binary\")\n",
    "         \n",
    "Multi-hot encoding data: where number of tokens is unknown\n",
    ">>> layer=CategoryEncoding(output_mode=\"binary\")\n",
    ">>> layer.adapt(sample_dataset)  # Indexes the vocabulary of the data\n",
    ">>> outputs=layer(inputs)\n",
    "                             \n",
    "Using weighted inputs in count mode:\n",
    ">>> layer=tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=4,output_mode=\"count\")\n",
    ">>> count_weights=np.array([[.1,.2],[.1,.1],[.2,.3],[.4,.2]])\n",
    ">>> layer([[0,1],[0,0],[1,2],[3,1]],count_weights=count_weights)                                                            \n",
    "\n",
    "Input:2D tensor(samples,timesteps)\n",
    "count_weights: 2D tensor in same shape as inputs indicating weight for each sample value when summing up in count mode. (Not used in binary|tfidf mode)\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "ii) Hashing: Implements categorical feature hashing, also known as \"hashing trick\"\n",
    " - Transforms single|multiple categorical inputs to hashed output & Converts sequence of int|str to sequence of int(hash function uses tensorflow::ops::Fingerprint to produce universal output,i.e. consistent across platforms)\n",
    " - Uses 'FarmHash64' by default,which provides consistent hashed output across different platforms & stable across invocations, Regardless of device & context, by mixing input bits thoroughly\n",
    " -    \n",
    "\n",
    "@ tf.keras.layers.experimental.preprocessing.Hashing(num_bins,salt=None,name=None)    \n",
    " * num_bins: Number of hash bins\n",
    " * salt:(1|2 unsigned int|None)If passed,hash function used will be SipHash64, with these values used as additional input (known as a \"salt\" in cryptography)(Must be non-zero)\n",
    "    \n",
    ">>> layer=tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3)\n",
    ">>> inp_1=[['A'],['B'],['C'],['D'],['E']]\n",
    ">>> inp_2=np.asarray([[5],[4],[3],[2],[1]])\n",
    ">>> layer([inp_1,inp_2])\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "iii) Discretization : Buckets data into discrete ranges\n",
    "    \n",
    "- Place each element of its input data into one of several contiguous ranges & output integer index indicating which range each element was placed in\n",
    "@ tf.keras.layers.experimental.preprocessing.Discretization(bins,epsilon=0.01)\n",
    " * bins: boundary specification|number of bins to compute if int\n",
    " * epsilon: Error tolerance, typically small fraction close to zero & Higher values of epsilon increase quantile approximation \n",
    "\n",
    ">>> layer=tf.keras.layers.experimental.preprocessing.Discretization(bins=[0.,1.,2.])\n",
    "\n",
    "Input: Any tf.Tensor | tf.RaggedTensor of dimension 2 or higher.\n",
    "Output: Same as input shape.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "iv) StringLookup layer\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "v) IntegerLookup :\n",
    "\n",
    "    \n",
    "------------------------------------------------------------------------------------------------------------\n",
    "vi) CategoryCrossing:  Concatenates 'multiple categorical inputs' into 'single categorical output'\n",
    "    - Output dtype is 'string'\n",
    "@ tf.keras.layers.experimental.preprocessing.CategoryCrossing(depth=None,name=None,separator=None)\n",
    " * depth: depth of input crossing & all inputs are crossed into one output\n",
    " * separator:(str) Added between each input being joined\n",
    "\n",
    ">>> layer=tf.keras.layers.experimental.preprocessing.CategoryCrossing(separator='-')\n",
    ">>> layer([inp_1,inp_2])\n",
    "\n",
    "## Image preprocessing & augmentation layers\n",
    "\n",
    "**i) Resizing layer:** Resize batched image input to target height & width.\n",
    "\n",
    "**ii) Rescaling layer:** Rescaling is applied both during training & inference\n",
    "\n",
    "**iii) CenterCrop layer:** Crop central portion of images to target height & width\n",
    "\n",
    "**iv) RandomCrop layer :** Randomly crop images to target height & width\n",
    "    \n",
    "**v) RandomFlip layer:** Randomly flip each image horizontally and vertically.\n",
    "\n",
    "**vi) RandomTranslation layer :** Randomly translate each image during training  \n",
    "\n",
    "**vii) RandomRotation layer:** Randomly rotate each image,By default, random rotations are only applied during training\n",
    "    \n",
    "**viii) RandomZoom layer:** Randomly zoom each image during training\n",
    "\n",
    "**ix) RandomHeight layer:** Randomly vary height of batch ,Adjusts height of  batch of images by random factor during training(inactive during inference)\n",
    "\n",
    "**x) RandomWidth layer:** Randomly vary width of batch ,Adjusts height of  batch of images by random factor during training(inactive during inference)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i) Resizing layer: Resize batched image input to target height & width.\n",
    "\n",
    "@ tf.keras.layers.experimental.preprocessing.Resizing(height,width,interpolation=\"bilinear\",name=None)\n",
    "  *height:(int)height of output shape.\n",
    "  *width:(int)width of output shape.\n",
    "  *interpolation: interpolation method{\"bilinear\"|\"nearest\"|\"bicubic\"|\"area\"|\"lanczos3\"|\"lanczos5\"|\"gaussian\"|\"mitchellcubic\"}\n",
    "  *name: Name of layer\n",
    "    \n",
    ">>>\n",
    "Input Shape: should be '4-D tensor(Name,height,width,channels)' in format of NHWC\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "ii) Rescaling layer: Rescaling is applied both during training & inference\n",
    "- Multiply inputs by scale & adds offset\n",
    "\n",
    "@ tf.keras.layers.experimental.preprocessing.Rescaling(scale,offset=0.0,name=None)\n",
    "    *scale: scale to apply to inputs\n",
    "        scale=1./255. : Rescale input[0,255] to [0,1] \n",
    "        scale=1./127.5,offset=-1 : Rescale input[0,255] to [-1,1]\n",
    "    *offset: offset to apply to inputs\n",
    "\n",
    ">>>\n",
    "Input & Output Shape: Arbitrary\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "iii) CenterCrop layer: Crop central portion of images to target height & width\n",
    "- If input height/width is even & target height/width is odd (or inversely),input image is left-padded by 1 pixel\n",
    "\n",
    "@ tf.keras.layers.experimental.preprocessing.CenterCrop(height,width,name=None)\n",
    "\n",
    ">>>\n",
    "\n",
    "Input shape: 4D tensor with shape(samples,height,width,channels) & data_format='channels_last'\n",
    "Output shape: 4D tensor with shape(samples,target_height,target_width,channels)\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "iv) RandomCrop layer : Randomly crop images to target height & width\n",
    "-Crop images in same batch to same cropping location,By default,It applied only during training \n",
    "-At inference time,images will be first rescaled to preserve shorter side & center cropped\n",
    "-If need to apply random cropping at inference time, set training to True when calling layer\n",
    "\n",
    "@ tf.keras.layers.experimental.preprocessing.RandomCrop(height,width,seed=None,name=None)\n",
    "    * seed:(int) Used to create a random seed\n",
    "\n",
    ">>> \n",
    "\n",
    "Input shape: 4D tensor with shape(samples,height,width,channels) & data_format='channels_last'.\n",
    "Output shape: 4D tensor with shape(samples,target_height,target_width,channels)\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "v) RandomFlip layer: Randomly flip each image horizontally and vertically. \n",
    "\n",
    "- Flip images based on mode attribute,inference & output will be identical to input\n",
    "- Call the layer with 'training=True' to flip input\n",
    "\n",
    "@ tf.keras.layers.experimental.preprocessing.RandomFlip(mode=\"horizontal_and_vertical\",seed=None,name=None)\n",
    "  * mode:\"horizontal\"|\"vertical\"|\"horizontal_and_vertical\"\n",
    "\n",
    ">>> \n",
    "\n",
    "Input shape: 4D tensor with shape(samples,height,width,channels) & data_format='channels_last'.\n",
    "Output shape: 4D tensor with shape(samples,target_height,target_width,channels) & data_format='channels_last'\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "vi) RandomTranslation layer : Randomly translate each image during training  \n",
    "\n",
    "@ tf.keras.layers.experimental.preprocessing.RandomTranslation(height_factor,width_factor,fill_mode=\"reflect\",interpolation=\"bilinear\",seed=None,name=None)\n",
    " * height_factor:(float|tuple) Representing lower & upper bound for shifting vertically\n",
    " * width_factor:(float|tuple) Representing lower & upper bound for shifting horizontally\n",
    " * fill_mode:Points outside boundaries of input are filled according to the given mode{'constant','reflect','wrap','nearest'}\n",
    "   reflect:(d c b a | a b c d | d c b a) input extended by reflecting about edge of last pixel\n",
    "   constant: (k k k k | a b c d | k k k k)input extended by filling all values beyond edge with same constant value k=0\n",
    "   wrap: (a b c d | a b c d | a b c d) input extended by wrapping around to opposite edge\n",
    "   nearest: (a a a a | a b c d | d d d d) input extended by nearest pixel\n",
    " * fill_value:(float) value to be filled outside boundaries when ' fill_mode is \"constant\" '\n",
    " * interpolation: horizontal\"nearest\"|\"bilinear\"\n",
    " \n",
    ">>> \n",
    "\n",
    "Input shape: 4D tensor with shape(samples,height,width,channels) & data_format='channels_last'.\n",
    "Output shape: 4D tensor with shape(samples,target_height,target_width,channels) & data_format='channels_last'\n",
    "Exception: ValueError: if either bound is not between [0,1] or upper bound < lower bound\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "vii) RandomRotation layer: Randomly rotate each image,By default, random rotations are only applied during training\n",
    "-If you need to apply random rotations at inference time, set \"training to True\" when calling layer\n",
    "\n",
    "@ tf.keras.layers.experimental.preprocessing.RandomRotation(factor,fill_mode=\"reflect\",interpolation=\"bilinear\",seed=None,name=None)\n",
    " * factor:(+ve float or tuple of size 2 representing lower & upper bound) Used for both upper & lower bound for rotating clockwise & counter-clockwise\n",
    "\n",
    ">>> \n",
    "\n",
    "Input shape: 4D tensor with shape(samples,height,width,channels) & data_format='channels_last'.\n",
    "Output shape: 4D tensor with shape(samples,target_height,target_width,channels) & data_format='channels_last'\n",
    "Exception: ValueError: if either bound is not between [0,1] or upper bound < lower bound\n",
    " \n",
    "---------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "viii) RandomZoom layer: Randomly zoom each image during training\n",
    "\n",
    "@ tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor,width_factor=None,fill_mode=\"reflect\",interpolation=\"bilinear\",seed=None,name=None)\n",
    " * height_factor:(float|tuple) Representing lower & upper bound for zooming vertically\n",
    " * width_factor:(float|tuple) Representing lower & upper bound for zooming horizontally\n",
    "\n",
    ">>> \n",
    "\n",
    "Input shape: 4D tensor with shape(samples,height,width,channels) & data_format='channels_last'.\n",
    "Output shape: 4D tensor with shape(samples,target_height,target_width,channels) & data_format='channels_last'\n",
    "Exception: ValueError: if either bound is not between [0,1] or upper bound=-ve\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "ix) RandomHeight layer: Randomly vary height of batch ,Adjusts height of  batch of images by random factor during training\n",
    "\n",
    "@ tf.keras.layers.experimental.preprocessing.RandomHeight(factor,interpolation=\"bilinear\",seed=None,name=None)\n",
    " * factor:(+ve float or tuple of size 2 representing lower & upper bound) Used for both upper & lower bound \n",
    " * interpolation:(str)\"bilinear\",\"nearest\",\"bicubic\",\"area\",\"lanczos3\",\"lanczos5\",\"gaussian\",\"mitchellcubic\"\n",
    "    \n",
    ">>>  \n",
    "\n",
    "Input shape: 4D tensor with shape(samples,height,width,channels) & data_format='channels_last'.\n",
    "Output shape: 4D tensor with shape(samples,target_height,target_width,channels) & data_format='channels_last'\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "x) RandomWidth layer: Randomly vary width of batch ,Adjusts height of  batch of images by random factor during training(inactive during inference)\n",
    "\n",
    "@ tf.keras.layers.experimental.preprocessing.RandomWidth(factor,interpolation=\"bilinear\",seed=None,name=None)\n",
    "\n",
    ">>>  \n",
    "\n",
    "Input shape: 4D tensor with shape(samples,height,width,channels) & data_format='channels_last'.\n",
    "Output shape: 4D tensor with shape(samples,height,random_width,channels) \n",
    "     \n",
    "\n",
    "# Core layers :\n",
    "\n",
    "**A) Input Layer :** Used to Instantiate Keras tensor\n",
    "\n",
    "https://keras.io/api/layers/core_layers/input/\n",
    "\n",
    "**B) Dense layer :**\n",
    "\n",
    "https://keras.io/api/layers/core_layers/dense/\n",
    "\n",
    "**C) :**\n",
    "\n",
    "\n",
    "A) Input() : Used to Instantiate Keras tensor\n",
    "\n",
    "@ tf.keras.Input(shape=None,batch_size=None,name=None,dtype=None,sparse=False,tensor=None,ragged=False)\n",
    " * shape:tuple(int) For instance,expected input shape=(32,) \n",
    " ** batch_size:(int) static batch size \n",
    " ** name:(str) Name for the layer\n",
    " * dtype:{float32|float64|int32} data type expected by input\n",
    " * sparse:(bool)whether placeholder to be created is sparse\n",
    " * ragged:(bool)whether placeholder to be created is ragged\n",
    " ** tensor:existing tensor to wrap into Input layer\n",
    "\n",
    ">>> x=Input(shape=(32,))\n",
    ">>> y=Dense(16,activation=\"relu\")(x)\n",
    "\n",
    "Raises :\n",
    "ValueError: If both sparse & ragged are provided.\n",
    "ValueError: If both shape & (batch_input_shape or batch_shape) are provided.\n",
    "ValueError: If both shape & tensor are None.\n",
    "ValueError: if any unrecognized parameters are provided\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    ") Embedding layer() ----------\n",
    "## Embedding layer('tf.keras.layers.Embedding()'): Turns +ve ints(indexes) into dense vectors of fixed (Only used in 1st layer of model)\n",
    "@ model.Embedding(input_dim,output_dim,embeddings_initializer=\"uniform\",embeddings_regularizer=None,activity_regularizer=None,embeddings_constraint=None,mask_zero=False,input_length=None)\n",
    " * input_dim:(Int)Size of vocabulary(max int index+1)\n",
    " * output_dim:(Int)Dim. of dense embedding\n",
    " * embeddings_initializer:Initializer for embeddings matrix\n",
    " * embeddings_regularizer:Regularizer function applied to embeddings matrix\n",
    " * embeddings_constraint: Constraint function applied to embeddings matrix\n",
    " * mask_zero:(bool)whether or not input value 0 is special \"padding\" value that should masked out\n",
    " * input_length: Length of input sequences, when it is constant\n",
    ">>> model.add(Embedding(voc_size,embedding_vector_features,input_length=20))  \n",
    "# Input shape: (batch_size,input_length)\n",
    "# Output shape: (batch_size,input_length,output_dim)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "---------- B.2.2.c) Lambda layer ---------- \n",
    "\n",
    "## Lambda layer('tf.keras.layers.Lambda()'): Wraps arbitrary expressions as Layer object\n",
    "@ tf.keras.layers.Lambda(function,output_shape=None,mask=None,arguments=None)\n",
    " * function:function to be evaluated(Takes input tensor as first argument)\n",
    " * output_shape:Expected output shape from function\n",
    " * mask:Either None(indicating no masking) | callable with same signature as compute_mask layer method, or  tensor that will be returned as output mask regardless of what input is\n",
    " ** arguments: dictionary of keyword arguments to be passed to function\n",
    "\n",
    ">>> \n",
    "# Input shape: Arbitrary\n",
    "# Output shape: Specified by 'output_shape'argument\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    " ---------- B.2.2.d) Masking layer ----------\n",
    "## Masking layer('tf.keras.layers.Masking()'): Masks sequence by using 'mask value' to skip timesteps\n",
    " \n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Layer Weight\n",
    "\n",
    "\n",
    "## Layer weight initializers\n",
    "- Initializers define way to set 'initial' random weights of Keras layers \n",
    "\n",
    "**keyword used:**\n",
    "- kernel_initializer :weight initialization\n",
    "- bias_initializer : bias initialization\n",
    "\n",
    "https://keras.io/api/layers/initializers/\n",
    "    \n",
    "**Available initializers**  \n",
    "***A) RandomNormal() :*** Initializer,generates tensors with 'normal distribution'\n",
    "\n",
    "***B) RandomUniform() :*** Initializer,generates tensors with 'uniform distribution'\n",
    "\n",
    "***C) TruncatedNormal() :*** Initializer,generates 'truncated normal distribution'\n",
    "    \n",
    "***D) Zeros() :*** Initializer,generates tensors initialized to 0.\n",
    "    \n",
    "***E) Ones() :*** Initializer,generates tensors initialized to 1\n",
    "\n",
    "***F) GlorotNormal() :*** Draws samples from 'truncated normal distribution'\n",
    "\n",
    "***G) GlorotUniform() :*** Draws samples from 'uniform distribution' within [-limit,limit]\n",
    "    \n",
    "***H) Identity() :*** Initializer,generates identity matrix\n",
    "\n",
    "***I) Orthogonal() :*** Initializer,generates an 'orthogonal matrix'\n",
    "\n",
    "***J) Constant() :*** Initializer,generates tensors with constant values.\n",
    "\n",
    "***K) VarianceScaling():*** Initializer capable of adapting its scale to shape of weights tensors\n",
    "\n",
    "***L) Creating Custom Initializers***\n",
    "\n",
    "\n",
    "A) RandomNormal() |'random_normal': Initializer,generates tensors with 'normal distribution'\n",
    "@ tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.05,seed=None)\n",
    " * mean:Mean of random values to generate\n",
    " * stddev: Standard deviation of random values to generate\n",
    " * seed: Randomize shape & dtype(seed will always produce same random tensor)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "B) RandomUniform() |'random_uniform': Initializer,generates tensors with 'uniform distribution'\n",
    "@ tf.keras.initializers.RandomUniform(minval=-0.05,maxval=0.05,seed=None)\n",
    " * minval: Lower bound of range of random values to generate (inclusive)\n",
    " * maxval: Upper bound of range of random values to generate (exclusive)\n",
    " * seed: Randomize shape & dtype(seed will always produce same random tensor)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "  \n",
    "C) TruncatedNormal()|'truncated_normal': Initializer,generates 'truncated normal distribution'\n",
    "- Generate,similar value to 'RandomNormal initializer' but values more than 'two std' from mean are discarded & re-drawn\n",
    "\n",
    "@ tf.keras.initializers.TruncatedNormal(mean=0.0,stddev=0.05,seed=None)\n",
    " * mean:Mean of random values to generate\n",
    " * stddev: Standard deviation of random values to generate\n",
    " * seed: Randomize shape & dtype(seed will always produce same random tensor)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "D) Zeros()|'zeros' : Initializer,generates tensors initialized to 0.\n",
    "@ tf.keras.initializers.Zeros()\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "E) Ones()|'ones': Initializer,generates tensors initialized to 1\n",
    "@ tf.keras.initializers.Ones()\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "F) GlorotNormal()|'glorot_normal' : Also called 'Xavier normal initializer'\n",
    "- Draws samples from 'truncated normal distribution' centered on 0 with 'stddev=sqrt(2/(fan_in+fan_out))'\n",
    "    where fan_in: no. of i/p weight  & fan_out: no. of o/p weight\n",
    "\n",
    "@ tf.keras.initializers.GlorotNormal(seed=None)\n",
    " * seed: Randomize shape & dtype(seed will always produce same random tensor)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "G) GlorotUniform()|'glorot_uniform' : Also called 'Xavier uniform initializer'\n",
    "- Draws samples from 'uniform distribution' within [-limit,limit]\n",
    "   where 'limit=sqrt(6/(fan_in+fan_out))' & fan_in: no. of i/p weight & fan_out: no. of o/p weight\n",
    "\n",
    "@ tf.keras.initializers.GlorotNormal(seed=None)\n",
    " * seed: Randomize shape & dtype(seed will always produce same random tensor)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "H) Identity()|'identity' : Initializer,generates identity matrix & Only usable for generating '2D matrices'\n",
    "@ tf.keras.initializers.Identity(gain=1.0)\n",
    " * gain: Multiplicative factor to apply to identity matrix\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "I) Orthogonal()|'orthogonal' :Initializer,generates an 'orthogonal matrix'\n",
    "- If shape(2D):it is initialized with 'orthogonal matrix' obtained from 'QR decomposition' of matrix of random numbers drawn from normal dist.\n",
    "- If matrix has fewer rows than columns then o/p will have 'orthogonal rows'.Otherwise o/p will have 'orthogonal columns'\n",
    "- If shape(more than 2D) matrix of shape (shape[0]*...*shape[n-2],shape[n-1]) is initialized,\n",
    "  where n:length of shape vector\n",
    "\n",
    "@ tf.keras.initializers.Orthogonal(gain=1.0,seed=None)\n",
    " * gain: Multiplicative factor to apply to identity matrix\n",
    " * seed: Randomize shape & dtype(seed will always produce same random tensor)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "J) Constant()|'constant' : Initializer,generates tensors with constant values.\n",
    " -constant value provided must be convertible to dtype requested when calling initializer(only scalar allowed)\n",
    "\n",
    "@ tf.keras.initializers.Constant(value=0)\n",
    " * value: constant value(scalar)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "K) VarianceScaling()|'variance_scaling' : Initializer capable of adapting its scale to shape of weights tensors\n",
    " With distribution=\"truncated_normal\"|\"untruncated_normal\",samples drawn from 'truncated/untruncated normal distribution' with mean=0 & std(after truncation,if used)stddev=sqrt(scale/n)\n",
    "    where n is:\n",
    "          if mode=\"fan_in\": no. of i/p units in weight tensor \n",
    "          if mode=\"fan_out\": no. of o/p units \n",
    "          if mode=\"fan_avg\": avg of no. of i/p & o/p units \n",
    "With distribution=\"uniform\",samples are drawn from uniform distribution within [-limit,limit]where limit=sqrt(3*scale/n)\n",
    "\n",
    "@ tf.keras.initializers.VarianceScaling(scale=1.0,mode=\"fan_in\",distribution=\"truncated_normal\",seed=None)\n",
    " * scale: Scaling factor(+ve float)\n",
    " * mode:{\"fan_in\"|\"fan_out\"|\"fan_avg\"}.\n",
    " * distribution:{\"truncated_normal\"|\"untruncated_normal\"|\"uniform\"}Random distribution to use \n",
    " * seed: Randomize shape & dtype(seed will always produce same random tensor)\n",
    "\n",
    "\n",
    "## Layer Weight Regularizers\n",
    "**Regularizers:** \n",
    "- Apply penalties on 'layer parameters'|'layer activity' during optimization \n",
    "- Regularization,applied on 'per-layer basis'\n",
    "- Penalties,summed into loss function that 'network optimizes'\n",
    "    \n",
    "**Key words:** \n",
    "- kernel_regularizer: Penalty on layer's weight\n",
    "- bias_regularizer: penalty on layer's bias\n",
    "- activity_regularizer:penalty on layer's output\n",
    "\n",
    "\n",
    "**Available regularizers:**\n",
    "\n",
    "***A) l1|'l1' :*** Applies 'L1 regularization penalty'\n",
    "\n",
    "***B)  l2|'l2' :*** Applies 'L2 regularization penalty'\n",
    "\n",
    "***C) L1_L2|\"l1_l2\" :*** Applies both 'L1' & 'L2' penalties\n",
    "\n",
    "***D) Creating custom regularizers :***\n",
    "\n",
    "https://keras.io/api/layers/regularizers/\n",
    "\n",
    "\n",
    "***A) l1|'l1' :*** Applies 'L1 regularization penalty'\n",
    "- Computed:  loss=l1*reduce_sum(abs(x))\n",
    "@ tf.keras.regularizers.l1(l1=0.01,**kwargs)\n",
    " * l1:(float) L1 regularization factor\n",
    "\n",
    ">>> dense=tf.keras.layers.Dense(3,kernel_regularizer=regularizers.l1(),bias_regularizer=regularizers.l1(),activity_regularizer=regularizers.l1())\n",
    ">>> dense=tf.keras.layers.Dense(3,kernel_regularizer='l1',bias_regularizer=\"l1\",activity_regularizer='l1')\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "***B) l2|'l2' :*** Applies 'L2 regularization penalty'\n",
    "- Computed:  loss=l2*reduce_sum(square(x))\n",
    "@ tf.keras.regularizers.l2(l2=0.01,**kwargs)\n",
    " * l2:(float) L2 regularization factor\n",
    "\n",
    ">>> dense=tf.keras.layers.Dense(3,kernel_regularizer=regularizers.l2(),bias_regularizer=regularizers.l2(),activity_regularizer=regularizers.l2())\n",
    ">>> dense=tf.keras.layers.Dense(3,kernel_regularizer='l2',bias_regularizer=\"l2\",activity_regularizer='l2')\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "***C) L1_L2|\"l1_l2\" :*** Applies both 'L1' & 'L2' penalties\n",
    "@ tf.keras.regularizers.l2(l2=0.01,**kwargs)\n",
    " * l1:(float) L1 regularization factor\n",
    " * l2:(float) L2 regularization factor\n",
    "\n",
    ">>> dense=tf.keras.layers.Dense(kernel_regularizer=regularizers.l1_l2(),bias_regularizer=regularizers.l1_l2(),activity_regularizer=regularizers.l1_l2())\n",
    ">>> dense=tf.keras.layers.Dense(kernel_regularizer='l1_l2',bias_regularizer=\"l1_l2\",activity_regularizer='l1_l2')\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "***D) Creating Custom Regularizers :***\n",
    "\n",
    "\n",
    "## Weight Constraints\n",
    "- Allow to set constraints(non-negativity) on model parameters during training\n",
    "- These 'per-variable projection functions' applied to target after each gradient update(when using fit())\n",
    "\n",
    "**keyword :**\n",
    "- kernel_constraint: For 'weights'\n",
    "- bias_constraint: For 'bias'\n",
    "\n",
    "**Available regularizers:**\n",
    "\n",
    "***A) Max_norm|'max_norm' :*** Constrains 'weights' incident to each hidden unit to have 'norm <= desired value'\n",
    "\n",
    "***B) MinMaxNorm()|'min_norm' :*** Constrains 'weights' incident to each hidden unit to have 'norm' between 'lower bound' & 'upper bound'\n",
    "\n",
    "***C) Non_neg()|'non-neg' :***Constrains the weights to be 'non-negative' \n",
    "\n",
    "***D) UnitNorm()|'unit_norm' :*** Constrains 'weights' incident to each hidden unit to have 'norm <= desired value'\n",
    "\n",
    "***E) Radial_constraint()|'radial_constraint' :*** Constrains 'Conv2D kernel' weights to be same for each radius\n",
    "\n",
    "***F) Creating Custom Weight Constraints :***\n",
    "\n",
    "https://keras.io/api/layers/constraints/\n",
    "\n",
    "\n",
    "A) Max_norm|'max_norm' : Constrains 'weights' incident to each hidden unit to have 'norm <= desired value'\n",
    "@ Max_norm(max_value=2,axis=0)\n",
    " * max_value: max norm for incoming weights\n",
    " * axis:(int)axis along which to calculate weight norms\n",
    "\n",
    ">>> model.add(Dense(64,kernel_constraint=Max_norm(),bias_constraint=Max_norm() ))\n",
    ">>> model.add(Dense(64,kernel_constraint='max_norm',bias_constraint='max_norm'))\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "B) MinMaxNorm()|'min_norm' : Constrains 'weights' incident to each hidden unit to have 'norm' between 'lower bound' & 'upper bound'\n",
    "@ MinMaxNorm(min_value=0,max_value=2,rate=1.0,axis=0)\n",
    " * min_value: min norm for incoming weights\n",
    " * max_value: max norm for incoming weights\n",
    " * rate: Weights will be rescaled to '(1-rate) * norm + rate * norm.clip(min_value,max_value)'\n",
    " * axis:(int)axis along which to calculate weight norms\n",
    "\n",
    ">>> model.add(Dense(64,kernel_constraint=MinMaxNorm(),bias_constraint=MinMaxNorm() ))\n",
    ">>> model.add(Dense(64,kernel_constraint='min_max_norm',bias_constraint='min_max_norm'))\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "C) Non_neg()|'non-neg' : Constrains the weights to be 'non-negative' \n",
    "@ Non_neg()\n",
    "\n",
    ">>> model.add(Dense(64,kernel_constraint=Non_neg(),bias_constraint=Non_neg()))\n",
    ">>> model.add(Dense(64,kernel_constraint='non-neg',bias_constraint='non-neg'))\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "D) UnitNorm()|'unit_norm' : Constrains 'weights' incident to each hidden unit to have 'norm <= desired value'\n",
    "@ UnitNorm(axis=0)\n",
    " * axis:(int)axis along which to calculate weight norms\n",
    "\n",
    ">>> model.add(Dense(64,kernel_constraint=UnitNorm(),bias_constraint=UnitNorm()))\n",
    ">>> model.add(Dense(64,kernel_constraint='unit_norm',bias_constraint='unit_norm'))\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "E) Radial_constraint()|'radial_constraint' : Constrains 'Conv2D kernel' weights to be same for each radius\n",
    "@ Radial_constraint()\n",
    "\n",
    ">>> model.add(Dense(64,kernel_constraint=Radial_constraint(),bias_constraint=Radial_constraint() ))\n",
    ">>> model.add(Dense(64,kernel_constraint='radial_constraint',bias_constraint='radial_constraint' ))\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "F) Creating Custom Weight Constraints :\n",
    "\n",
    "\n",
    "\n",
    "# RNN Layers :\n",
    "\n",
    "\n",
    "\n",
    "RNN('tf.keras.layers.RNN()'):Base class for recurrent layers.\n",
    "@ RNN(cell,return_sequences=False,return_state=False,go_backwards=False,stateful=False,unroll=False,time_major=False)\n",
    " * cell:(RNN cell |list of RNN cell). A RNN cell is a class that has:\n",
    "A call(input_at_t,states_at_t) method, returning (output_at_t, states_at_t_plus_1). The call method of the cell can also take the optional argument constants, see section \"Note on passing external constants\" below.\n",
    "A state_size attribute. This can be a single integer (single state) in which case it is the size of the recurrent state. This can also be a list/tuple of integers (one size per state). The state_size can also be TensorShape or tuple/list of TensorShape, to represent high dimension state.\n",
    "A output_size attribute. This can be a single integer or a TensorShape, which represent the shape of the output. For backward compatible reason, if this attribute is not available for the cell, the value will be inferred by the first element of the state_size.\n",
    "A get_initial_state(inputs=None, batch_size=None, dtype=None) method that creates a tensor meant to be fed to call() as the initial state, if the user didn't specify any initial state via other means. The returned initial state should have a shape of [batch_size, cell.state_size]. The cell might choose to create a tensor full of zeros, or full of other values based on the cell's implementation. inputs is the input tensor to the RNN layer, which should contain the batch size as its shape[0], and also dtype. Note that the shape[0] might be None during the graph construction. Either the inputs or the pair of batch_size and dtype are provided. batch_size is a scalar tensor that represents the batch size of the inputs. dtype is tf.DType that represents the dtype of the inputs. For backward compatible reason, if this method is not implemented by the cell, the RNN layer will create a zero filled tensor with the size of [batch_size, cell.state_size]. In the case that cell is a list of RNN cell instances, the cells will be stacked on top of each other in the RNN, resulting in an efficient stacked RNN.\n",
    " * return_sequences:(bool)Whether to return last output in output sequence or full sequence\n",
    " * return_state:(bool) Whether to return last state in addition to output\n",
    " * go_backwards:(bool) If True,process input sequence backwards & return reversed sequence\n",
    " * stateful:(bool)If True,last state for each sample at index i in batch will be used as initial state for sample of index i in following batch\n",
    " * unroll:(bool) If True,network will be unrolled, else symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences.\n",
    " * time_major:shape format of inputs & outputs tensors. \n",
    "         =True:inputs & outputs will be in shape [timesteps,batch,...]  (efficient cuz,avoids transposes at beginning @ end of RNN calculation)\n",
    "         =False:inputs & outputs will be in shape [batch,timesteps,...] \n",
    " * zero_output_for_mask:(bool) Whether o/p,use zeros for masked timesteps(only used when 'return_sequences=True' & 'mask' provided)\n",
    "    \n",
    "# Call arguments\n",
    " * inputs: Input tensor.\n",
    " * mask: Binary tensor of shape [batch_size,timesteps] indicating whether given timestep should be masked.\n",
    " * training:(bool) whether layer should behave in training mode or in inference mode(used with cells that use dropout)\n",
    " * initial_state: List of initial state tensors to be passed to first call of the cell.\n",
    " * constants: List of constant tensors to be passed to cell at each timestep.\n",
    "# Input shape: n-D tensor of shape[batch_size,timesteps, ...] or [timesteps,batch_size, ...] when time_major is True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SimpleRNN('tf.keras.layers.SimpleRNN()') : Fully-connected RNN, where output is to be fed back to input\n",
    "@ SimpleRNN(units,activation=\"tanh\",use_bias=True,kernel_initializer=\"glorot_uniform\",recurrent_initializer=\"orthogonal\",bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,recurrent_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,\n",
    "    recurrent_constraint=None,bias_constraint=None,dropout=0.0,recurrent_dropout=0.0,return_sequences=False,return_state=False,\n",
    "    go_backwards=False,stateful=False,unroll=False,**kwargs)\n",
    " * units:(+ve int) dimensionality of output space\n",
    " * activation: Activation function to use\n",
    " * use_bias:(bool), whether layer uses bias vector\n",
    " * kernel_initializer,recurrent_initializer,bias_initializer: Initializer for 'kernel weights','recurrent_kernel weights','bias'\n",
    " * kernel_regularizer,recurrent_regularizer,bias_regularizer,activity_regularizer: Regularizer function applied to 'kernel weights','recurrent_kernel','bias','output of layer'(its \"activation\")\n",
    " * kernel_constraint,bias_constraint,recurrent_constraint: Constraint function for 'kernel weights','recurrent_kernel','bias' \n",
    " * dropout,recurrent_dropout:(float between 0-1) Fraction of units to drop from 'inputs','recurrent state'\n",
    " * return_sequences:(bool)Whether to return last output in output sequence or full sequence\n",
    " * return_state:(bool)Whether to return last state in addition to output\n",
    " * go_backwards:(bool)If True:process input sequence backwards & return reversed sequence\n",
    " * stateful:(bool)If True:last state for each sample at index i in batch will be used as initial state for sample of index i in following batch\n",
    " * unroll:(bool) If True:network will be unrolled, else symbolic loop will be used. Unrolling can speed-up RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences\n",
    "        \n",
    "# Call arguments:\n",
    " * inputs:3D tensor,shape[batch,timesteps,feature]\n",
    " * mask:Binary tensor of shape[batch,timesteps] indicating whether given timestep should be masked.\n",
    " * training: Python boolean indicating whether layer should behave in 'training mode'/'inference mode'(used if 'dropout'/'recurrent_dropout' is used)\n",
    " * initial_state: List of initial state tensors to be passed to first call of the cell\n",
    "## https://keras.io/api/layers/recurrent_layers/simple_rnn/   \n",
    "\n",
    "\n",
    "\n",
    "LSTM('tf.keras.layers.LSTM()'): Long Short-Term Memory layer\n",
    "- Based on (cuDNN-based or pure-TensorFlow) to maximize the performance\n",
    "       If GPU(cuDNN):available & all arguments to layer meet requirement of CuDNN kernel,then layer will use fast cuDNN implementation\n",
    "@ LSTM(units,activation=\"tanh\",recurrent_activation=\"sigmoid\",use_bias=True,kernel_initializer=\"glorot_uniform\",recurrent_initializer=\"orthogonal\",bias_initializer=\"zeros\",unit_forget_bias=True,kernel_regularizer=None,recurrent_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,recurrent_constraint=None,bias_constraint=None,dropout=0.0,recurrent_dropout=0.0,return_sequences=False,return_state=False,go_backwards=False,stateful=False,time_major=False,unroll=False)\n",
    " * units:(+ve int)dimensionality of output space\n",
    " * activation: Activation function to use\n",
    " * recurrent_activation: Activation function to use for recurrent step\n",
    " * use_bias:(bool)whether layer uses bias vector\n",
    " * kernel_initializer,recurrent_initializer,bias_initializer: Initializer 'kernel weights','recurrent_kernel weights','bias' \n",
    " * unit_forget_bias:(bool)If True:add 1 to bias of forget gate at initialization(force bias_initializer=\"zeros\")\n",
    " * kernel_regularizer,recurrent_regularizer,bias_regularizer,activity_regularizer:Regularizer applied to 'kernel weights','recurrent_kernel weights','bias','output of the layer'\n",
    " * kernel_constraint,recurrent_constraint,bias_constraint: Constraint applied to 'kernel weights','recurrent_kernel weights','bias'\n",
    " * dropout:(float  0-1)Fraction of units to drop for linear transformation of inputs\n",
    " * recurrent_dropout:(float  0-1)Fraction of units to drop for linear transformation of recurrent state\n",
    " * return_sequences:(bool)Whether to return last o\\p in output sequence or full sequence\n",
    " * return_state:(bool)Whether to return last state in addition to o\\p\n",
    " * go_backwards:(bool) True:process input sequence backwards & return reversed sequence.\n",
    " * stateful:(bool) True:last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    " * time_major:shape format of i/p & o/p tensors\n",
    "     True:inputs & outputs will be in shape[timesteps,batch,feature]   (more efficient because it avoids transposes at beginning & end of RNN calculation)\n",
    "     False:inputs & outputs will be in shape [batch,timesteps,feature]\n",
    " * unroll:(bool) If True: network will be unrolled,else symbolic loop will be used (Unrolling can speed-up a RNN but tends to be more memory-intensive)\n",
    "\n",
    "# Call arguments :\n",
    " * inputs:3D tensor,shape[batch,timesteps,feature]\n",
    " * mask:Binary tensor of shape[batch,timesteps] indicating whether given timestep should be masked.\n",
    " * training: Python boolean indicating whether layer should behave in 'training mode'/'inference mode'(used if 'dropout'/'recurrent_dropout' is used)\n",
    " * initial_state: List of initial state tensors to be passed to first call of the cell\n",
    "\n",
    "## https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "\n",
    "GRU('tf.keras.layers.GRU()'):Gated Recurrent Unit\n",
    "- variants of GRU implementation:\n",
    "     (default) one is based on v3 & has reset gate applied to hidden state before matrix multiplication\n",
    "     Other one is based on original & has order reversed(Gpu-only & allows inference on CPU)\n",
    "\n",
    "@ GRU(units,activation=\"tanh\",recurrent_activation=\"sigmoid\",use_bias=True,kernel_initializer=\"glorot_uniform\",recurrent_initializer=\"orthogonal\",bias_initializer=\"zeros\",kernel_regularizer=None,recurrent_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,recurrent_constraint=None,bias_constraint=None,dropout=0.0,recurrent_dropout=0.0,return_sequences=False,return_state=False,go_backwards=False,stateful=False,unroll=False,time_major=False,reset_after=True)\n",
    " * units:(+ve int)dimensionality of output space\n",
    " * activation: Activation function to use\n",
    " * recurrent_activation: Activation function to use for recurrent step\n",
    " * use_bias:(bool)whether layer uses bias vector\n",
    " * kernel_initializer,recurrent_initializer,bias_initializer: Initializer 'kernel weights','recurrent_kernel weights','bias' \n",
    " * unit_forget_bias:(bool)If True:add 1 to bias of forget gate at initialization(force bias_initializer=\"zeros\")\n",
    " * kernel_regularizer,recurrent_regularizer,bias_regularizer,activity_regularizer:Regularizer applied to 'kernel weights','recurrent_kernel weights','bias','output of the layer'\n",
    " * kernel_constraint,recurrent_constraint,bias_constraint: Constraint applied to 'kernel weights','recurrent_kernel weights','bias'\n",
    " * dropout:(float  0-1)Fraction of units to drop for linear transformation of inputs\n",
    " * recurrent_dropout:(float  0-1)Fraction of units to drop for linear transformation of recurrent state\n",
    " * return_sequences:(bool)Whether to return last o\\p in output sequence or full sequence\n",
    " * return_state:(bool)Whether to return last state in addition to o\\p\n",
    " * go_backwards:(bool) True:process input sequence backwards & return reversed sequence.\n",
    " * stateful:(bool) True:last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    " * time_major:shape format of i/p & o/p tensors\n",
    "     True:inputs & outputs will be in shape[timesteps,batch,feature]   (more efficient because it avoids transposes at beginning & end of RNN calculation)\n",
    "     False:inputs & outputs will be in shape [batch,timesteps,feature]\n",
    " * unroll:(bool) If True: network will be unrolled,else symbolic loop will be used (Unrolling can speed-up a RNN but tends to be more memory-intensive)\n",
    " * reset_after: GRU convention (whether to apply reset gate after or before matrix multiplication)\n",
    "             =False: \"before\"  & =True: \"after\" (default & CuDNN compatible)\n",
    "\n",
    "# Call arguments :\n",
    " * inputs:3D tensor,shape[batch,timesteps,feature]\n",
    " * mask:Binary tensor of shape[batch,timesteps] indicating whether given timestep should be masked.\n",
    " * training: Python boolean indicating whether layer should behave in 'training mode'/'inference mode'(used if 'dropout'/'recurrent_dropout' is used)\n",
    " * initial_state: List of initial state tensors to be passed to first call of the cell\n",
    "\n",
    "## https://keras.io/api/layers/recurrent_layers/gru/\n",
    "\n",
    "\n",
    "\n",
    "Bidirectional('tf.keras.layers.Bidirectional()'): Bidirectional wrapper for RNNs\n",
    "@ tf.keras.layers.Bidirectional(layer,merge_mode=\"concat\",weights=None,backward_layer=None)\n",
    " * layer=RNN instance: ex: LSTM|GRU Layer\n",
    "        =keras.layers.Layer : sequence layer(accepts 3D+ inputs), go_backwards,return_sequences & return_state attribute(with same semantics as for  RNN class)\n",
    "         Implement serialization via 'get_config()' & from_config()\n",
    " * merge_mode:{'sum','mul','concat','ave',None} Mode by which outputs of forward & backward RNNs will be combined \n",
    "             =None:outputs will not be combined,they will be returned as list\n",
    " ** backward_layer:keras.layers.RNN | keras.layers.Layer , to be used to handle backwards input processing\n",
    "     \n",
    "# Call arguments: same as those of wrapped RNN layer\n",
    "## https://keras.io/api/layers/recurrent_layers/bidirectional/\n",
    "\n",
    "\n",
    "# Normalization layers :\n",
    "**1) Batch Normalization :**\n",
    "- Layer that normalizes its inputs(Works differently during training & during inference)\n",
    "- Applies transformation,that mean output close to 0 & output std close to 1\n",
    "\n",
    "\n",
    "**During training**- while using fit() or when calling layer/model with argument \"training=True\"\n",
    "- layer normalizes its output using the mean & std of current batch of inputs\n",
    "- For each channel being normalized,layer returns \n",
    " (batch - mean(batch)) / (var(batch) + epsilon) * gamma + beta\n",
    " \n",
    "***epsilon :***small constant (configurable as part of constructor arguments)\n",
    "\n",
    "***gamma :*** learned scaling factor(initialized as 1), which can be disabled by passing \"scale=False\" to constructor.\n",
    "\n",
    "***beta :*** learned offset factor(initialized as 0), which can be disabled by passing \"center=False\" to constructor\n",
    "\n",
    "**During inference**- while using evaluate() or predict() or when calling the layer/model with the argument training=False \n",
    "- layer normalizes its output using moving average of mean & std of batches it has seen during training. \n",
    "- For each channel being normalized,returns \n",
    "(batch - self.moving_mean) / (self.moving_var + epsilon) * gamma + beta.\n",
    "\n",
    "- self.moving_mean & self.moving_var are non-trainable variables that are updated each time layer in called in training mode\n",
    "\n",
    "***moving_mean*** = moving_mean * momentum + mean(batch) * (1 - momentum)\n",
    "\n",
    "***moving_var*** = moving_var * momentum + var(batch) * (1 - momentum)\n",
    "\n",
    "https://keras.io/api/layers/normalization_layers/batch_normalization/\n",
    "\n",
    "**2) Layer Normalization layer:** \n",
    "- Normalize activations of previous layer for each given example in batch independently,rather than (across batch like Batch Normalization applies transformation that maintains mean to 0 & std to 1)\n",
    "- Given tensor inputs,moments are calculated & normalization is performed across axes specified in axis\n",
    "\n",
    "https://keras.io/api/layers/normalization_layers/layer_normalization/\n",
    "\n",
    "\n",
    "\n",
    "@ tf.keras.layers.BatchNormalization(axis=-1,momentum=0.99,epsilon=0.001,center=True,scale=True,\n",
    "    beta_initializer=\"zeros\",gamma_initializer=\"ones\",moving_mean_initializer=\"zeros\",moving_variance_initializer=\"ones\",\n",
    "    beta_regularizer=None,gamma_regularizer=None,beta_constraint=None,gamma_constraint=None)\n",
    "\n",
    " * axis:(int)Axis that should be normalized (typically features axis)\n",
    "    For instance,after Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
    " * momentum: Momentum for moving average.\n",
    " * epsilon: Small float added to variance to avoid dividing by zero.\n",
    " * center:If True=add offset of beta to normalized tensor\n",
    "          If False=beta is ignored.\n",
    " * scale:If True=multiply by gamma.\n",
    "         If False=gamma is not used. When next layer is linear(also e.g. nn.relu), this can be disabled since scaling will be done by next layer.\n",
    " * beta_initializer: Initializer for beta weight.\n",
    " * gamma_initializer: Initializer for gamma weight.\n",
    " * moving_mean_initializer: Initializer for moving mean.\n",
    " * moving_variance_initializer: Initializer for moving variance.\n",
    " ** beta_regularizer: regularizer for beta weight.\n",
    " ** gamma_regularizer: regularizer for gamma weight.\n",
    " ** beta_constraint: constraint for beta weight.\n",
    " ** gamma_constraint: constraint for gamma weight.\n",
    "\n",
    "\n",
    "@ tf.keras.layers.LayerNormalization(axis=-1,epsilon=0.001,center=True,scale=True,\n",
    "    beta_initializer=\"zeros\",gamma_initializer=\"ones\",\n",
    "    beta_regularizer=None,gamma_regularizer=None,beta_constraint=None,gamma_constraint=None)\n",
    "\n",
    " * axis:(int|List|Tuple)axis or axes to normalize across(features axis) \n",
    "        left-out axes are typically the batch axis/axes.\n",
    "        last dimension in input(axis=-1)\n",
    " * epsilon: Small float added to variance to avoid dividing by zero. Defaults to 1e-3\n",
    " * center: If True=add offset of beta to normalized tensor.\n",
    "           If False=beta is ignored\n",
    " * scale: If True=multiply by gamma. \n",
    "          If False=gamma is not used  When next layer is linear(also e.g. nn.relu), this can be disabled since scaling will be done by next layer\n",
    " * beta_initializer: Initializer for beta weight\n",
    " * gamma_initializer: Initializer for gamma weight\n",
    " ** beta_regularizer:regularizer for beta weight\n",
    " ** gamma_regularizer:regularizer for gamma weight\n",
    " ** beta_constraint:constraint for beta weight\n",
    " ** gamma_constraint:constraint for gamma weight\n",
    "\n",
    "\n",
    "\n",
    "# Regularization layers: \n",
    "\n",
    "**A) Dropout layer :** Randomly \n",
    "\n",
    "https://keras.io/api/layers/regularization_layers/dropout/\n",
    "\n",
    "**B) Spatial Dropout layer :**\n",
    "\n",
    "https://keras.io/api/layers/regularization_layers/spatial_dropout1d/\n",
    "\n",
    "https://keras.io/api/layers/regularization_layers/spatial_dropout2d/\n",
    "\n",
    "https://keras.io/api/layers/regularization_layers/spatial_dropout3d/\n",
    "\n",
    "**C) Gaussian Dropout layer :**\n",
    "\n",
    "https://keras.io/api/layers/regularization_layers/gaussian_dropout/\n",
    "\n",
    "\n",
    "**D) Gaussian Noise layer :**\n",
    "\n",
    "https://keras.io/api/layers/regularization_layers/gaussian_noise/\n",
    "\n",
    "**E) Activity Regularization layer :**\n",
    "\n",
    "https://keras.io/api/layers/regularization_layers/activity_regularization/\n",
    "\n",
    "**F) Alpha Dropout layer :**\n",
    "\n",
    "https://keras.io/api/layers/regularization_layers/alpha_dropout/\n",
    "\n",
    "\n",
    "A) Dropout layer : \n",
    " - Randomly sets i/p node to 0 with frequency of rate at each step during training time,helps prevent overfitting\n",
    " - Inputs not set to 0,scaled up by 1/(1-rate) such that sum over all inputs is unchanged\n",
    " - only applies when training is True such that no values dropped during inference\n",
    " - When using 'model.fit',training set to True automatically\n",
    "\n",
    "@ Dropout(rate,noise_shape=None,seed=None)\n",
    " * rate:(Float between 0-1) Fraction of input units to drop\n",
    " * noise_shape:(1D int tensor) shape of binary dropout mask that will be multiplied with input\n",
    " * seed:(int) random seed\n",
    "\n",
    ">>> model.add(Dropout(0.5))\n",
    "\n",
    "Call arguments:\n",
    "  inputs: Input tensor(any rank)\n",
    "  training:(bool)whether layer should behave in training mode(adding dropout) or in inference mode(doing nothing)\n",
    "    \n",
    "\n",
    "\n",
    "B) Spatial Dropout layer:\n",
    "    \n",
    "B|i) SpatialDropout1D layer()\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "B|ii) SpatialDropout2D layer(): \n",
    " - Performs same as Dropout,but it drops entire 2D feature maps instead of individual elements.\n",
    " - If adjacent pixels within feature maps are strongly correlated(as is normally case in early convolution layers)then regular dropout will not regularize activations & will otherwise just result in effective learning rate decrease. \n",
    " - SpatialDropout2D will help promote independence between feature maps & should be used instead\n",
    "\n",
    "@ tf.keras.layers.SpatialDropout2D(rate,data_format=None)\n",
    " * rate:(Float between 0-1) Fraction of input units to drop\n",
    " * data_format='channels_last'(default):inputs with shape (batch,height,width,channels)\n",
    "             ='channels_first': inputs with shape (batch,channels,height,width) \n",
    "\n",
    ">>>\n",
    "\n",
    "Call Arguments:\n",
    "  inputs:4D tensor\n",
    "  training:(bool)whether layer should behave in training mode(adding dropout) or in inference mode (doing nothing).\n",
    "\n",
    "Input shape: if data_format='channels_first': 4D tensor (Batch,Channel,Height,Width)  \n",
    "               if data_format='channels_last': 4D tensor (Batch,Height,Width,Channel)\n",
    "Output shape: Same as input\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "B|iii) SpatialDropout3D layer()\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "C) Gaussian Dropout layer :\n",
    "\n",
    "\n",
    "\n",
    "D) GaussianNoise layer :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "E) ActivityRegularization layer():\n",
    "- Layer applies update to cost function based input activity\n",
    "\n",
    "@ tf.keras.layers.ActivityRegularization(l1=0.0,l2=0.0)\n",
    " * l1:(+ve float) L1 regularization factor \n",
    " * l2:(+ve float) L2 regularization factor\n",
    "\n",
    "Input shape:\n",
    "  input_shape (tuple of int,does not include samples axis) when using this layer as first layer in model\n",
    "\n",
    "Output shape: Same shape as input\n",
    "\n",
    "\n",
    "\n",
    "F) AlphaDropout layer :\n",
    "\n",
    "\n",
    "\n",
    "# Reshaping layers :\n",
    "\n",
    "**A) Flatten layer :** Flattens input & Does not affect batch size\n",
    "\n",
    "https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "\n",
    "**B) Reshape layer :**\n",
    "\n",
    "https://keras.io/api/layers/reshaping_layers/reshape/\n",
    "\n",
    "\n",
    "**C) Zero Padding Layer :**\n",
    "ZeroPadding1D layer\n",
    "\n",
    "ZeroPadding2D layer\n",
    "\n",
    "https://keras.io/api/layers/reshaping_layers/zero_padding2d/\n",
    "\n",
    "ZeroPadding3D layer\n",
    "\n",
    "**D) Cropping Layer :** \n",
    "Cropping1D layer\n",
    "Cropping2D layer\n",
    "\n",
    "https://keras.io/api/layers/reshaping_layers/cropping2d/\n",
    "\n",
    "Cropping3D layer\n",
    "\n",
    "\n",
    "**E) UpSampling Layer :**  \n",
    "UpSampling1D layer :\n",
    "\n",
    "UpSampling2D layer:\n",
    "\n",
    "https://keras.io/api/layers/reshaping_layers/up_sampling2d/\n",
    "\n",
    "UpSampling3D layer :\n",
    "\n",
    "**F) RepeatVector layer :** Repeats input \"n\" times\n",
    "\n",
    "https://keras.io/api/layers/reshaping_layers/repeat_vector/\n",
    "\n",
    "**G) Permute layer :** Permutes dimensions of input according to given pattern\n",
    "\n",
    "https://keras.io/api/layers/reshaping_layers/permute/\n",
    "\n",
    "\n",
    "A) Flatten layer:\n",
    "- Flattens input & Does not affect batch size\n",
    "\n",
    "@ tf.keras.layers.Flatten(data_format=None)\n",
    " * data_format='channels_last'(default):inputs with shape (batch,height,width,channels)\n",
    "              ='channels_first': inputs with shape (batch,channels,height,width) \n",
    "\n",
    ">>> model.add(Flatten())          # Applied before 'Dense Layer' & after 'Convolutional layer'  \n",
    "\n",
    "Note:If inputs shape (batch,) without feature axis,\n",
    "    then flattening adds an extra channel dimension & output shape(batch,1)\n",
    "\n",
    "\n",
    "\n",
    "B) Reshape layer:\n",
    "- Reshapes inputs into given shape\n",
    "\n",
    "@ tf.keras.layers.Reshape(target_shape)\n",
    " * target_shape:\n",
    "    \n",
    ">>> model.add(tf.keras.layers.Reshape((6,2)))\n",
    "\n",
    "Input shape : dims in input shape must be known|fixed 'input_shape'(tuple of int) when using this layer as first layer in model\n",
    "Output shape : (batch_size,) + target_shape\n",
    "\n",
    "\n",
    "C) Zero Padding Layer :\n",
    "    \n",
    "C|i) ZeroPadding1D layer\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "C|ii) ZeroPadding2D layer(): Add rows & columns of 'zeros' at top,bottom,left,right side of an image tensor\n",
    "\n",
    "@ tf.keras.layers.ZeroPadding2D(padding,data_format=None)\n",
    " * cropping:(int/tuple of 2 int/tuple of 2 tuples of 2 ints)\n",
    "      If int: same symmetric padding is applied to height & width.\n",
    "      If tuple of 2 int: interpreted as two different symmetric padding values for height & width\n",
    "      If tuple of 2 tuples of 2 ints: interpreted as ((top_pad,bottom_pad),(left_pad,right_pad))\n",
    " * data_format='channels_last'(default):inputs with shape (batch,height,width,channels)\n",
    "             ='channels_first': inputs with shape (batch,channels,height,width) \n",
    "\n",
    ">>> y=tf.keras.layers.ZeroPadding2D(padding=1)(x)\n",
    "\n",
    "Input shape: if data_format='channels_first': 4D tensor (Batch,Channel,Height,Width)  \n",
    "               if data_format='channels_last': 4D tensor (Batch,Height,Width,Channel)\n",
    "Output shape:if data_format='channels_first':4D tensor (Batch,Filter,padded_Height,padded_Width)  \n",
    "               if data_format='channels_last':4D tensor (Batch,padded_Height,padded_Width,Filter)\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "C|iii) ZeroPadding3D layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "D) Cropping Layer :\n",
    "    \n",
    "D|i) Cropping1D layer\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "D|ii) Cropping2D layer()\n",
    " - Cropping layer for 2D input(e.g. picture)\n",
    " - It crops along spatial dimensions(height & width)\n",
    "\n",
    "@ tf.keras.layers.Cropping2D(cropping,data_format)\n",
    " * cropping:(int/tuple of 2 int/tuple of 2 tuples of 2 ints)\n",
    "      If int: same symmetric cropping is applied to height & width.\n",
    "      If tuple of 2 int: interpreted as two different symmetric cropping values for height & width\n",
    "      If tuple of 2 tuples of 2 ints: interpreted as ((top_crop,bottom_crop),(left_crop,right_crop))\n",
    " * data_format='channels_last'(default):inputs with shape (batch,height,width,channels)\n",
    "             ='channels_first': inputs with shape (batch,channels,height,width) \n",
    "\n",
    ">>> y=tf.keras.layers.Cropping2D(cropping=((2,2),(4,4)))(x)\n",
    "                                    \n",
    "Input shape: If data_format='channels_last': 4D tensor with shape(batch_size,rows,cols,channels)\n",
    "                data_format='channels_first': 4D tensor with shape(batch_size,channels,rows,cols)\n",
    "Out shape: If data_format='channels_last': 4D tensor with shape(batch_size,cropped_rows,cropped_cols,channels)\n",
    "              data_format='channels_first': 4D tensor with shape(batch_size,channels,cropped_rows,cropped_cols)\n",
    "                                    \n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "D|iii) Cropping3D layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "E) UpSampling Layer :\n",
    "    \n",
    "E|i) UpSampling1D layer\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "E|ii) UpSampling2D layer()\n",
    " - Upsampling layer for 2D inputs.\n",
    " - Repeats rows & columns of data by size[0] & size[1] respectively\n",
    "\n",
    "@ tf.keras.layers.UpSampling2D(size=(2,2),data_format=None,interpolation=\"nearest\")\n",
    " * size:(int/ tuple of 2 int)upsampling factors for rows & columns.\n",
    " * interpolation:('str'), one of nearest / bilinear\n",
    " * data_format='channels_last'(default):inputs with shape (batch,height,width,channels)\n",
    "             ='channels_first': inputs with shape (batch,channels,height,width) \n",
    "\n",
    ">>> y=tf.keras.layers.UpSampling2D(size=(1,2))(x)\n",
    "            \n",
    "Input shape: If data_format='channels_last': 4D tensor with shape(batch_size,rows,cols,channels)\n",
    "                data_format='channels_first': 4D tensor with shape(batch_size,channels,rows,cols)\n",
    "Out shape: If data_format='channels_last': 4D tensor with shape(batch_size,upsampled_rows,upsampled_cols,channels)\n",
    "              data_format='channels_first': 4D tensor with shape(batch_size,channels,upsampled_rows,upsampled_cols)\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "E|iii) UpSampling3D layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "F) RepeatVector layer :\n",
    "- Repeats input \"n\" times\n",
    "\n",
    "@ tf.keras.layers.RepeatVector(n)\n",
    " * n:(int) repetition factor\n",
    "\n",
    ">>> model.add(RepeatVector(3))\n",
    "\n",
    "Input shape: 2D tensor of shape(num_samples,features)\n",
    "Output shape: 3D tensor of shape(num_samples,n,features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "G) Permute layer : \n",
    "- Permutes dimensions of input according to given pattern.\n",
    "- Useful connecting RNNs & convnets\n",
    "\n",
    "@ tf.keras.layers.Permute(dims)\n",
    " * dims:(Tuple of int) Permutation pattern does not include samples dimension, indexing starts at 1\n",
    "\n",
    "        \n",
    ">>> model = Sequential()\n",
    ">>> model.add(Permute((2, 1), input_shape=(10, 64)))\n",
    "\n",
    "Input shape:    \n",
    "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis)when using this layer as first layer in model\n",
    "\n",
    "Output shape :\n",
    "Same as input shape,but with dimensions re-ordered according to specified pattern\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merging layers :\n",
    "**A) Concatenate layer :** Concatenates list of inputs\n",
    "\n",
    "https://keras.io/api/layers/merging_layers/concatenate/\n",
    "\n",
    "**B) Average layer :** Averages list of inputs element-wise\n",
    "\n",
    "https://keras.io/api/layers/merging_layers/average/\n",
    "\n",
    "**C) Maximum layer :** Computes maximum(element-wise) a list of inputs\n",
    "\n",
    "https://keras.io/api/layers/merging_layers/maximum/\n",
    "\n",
    "**D) Minimum layer :** Computes minimum(element-wise) a list of inputs\n",
    "\n",
    "https://keras.io/api/layers/merging_layers/minimum/\n",
    "\n",
    "**E) Add layer :** Layer that adds a list of inputs\n",
    "\n",
    "https://keras.io/api/layers/merging_layers/add/\n",
    "\n",
    "**F) Subtract layer :** Subtracts two inputs\n",
    "\n",
    "https://keras.io/api/layers/merging_layers/subtract/\n",
    "\n",
    "**G) Multiply layer :** Multiplies (element-wise) a list of inputs\n",
    "\n",
    "https://keras.io/api/layers/merging_layers/multiply/\n",
    "\n",
    "**H) Dot layer :** Computes dot product between samples in two tensors           \n",
    "https://keras.io/api/layers/merging_layers/dot/\n",
    "\n",
    "\n",
    "\n",
    "A) Concatenate layer :\n",
    "- Layer that concatenates a list of inputs.\n",
    "- It takes input(list of tensors), all of same shape except for concatenation axis,returns single tensor that is concatenation of all inputs\n",
    "\n",
    "@ tf.keras.layers.Concatenate(axis=-1)\n",
    " * axis:\n",
    "\n",
    ">>> concatted=tf.keras.layers.Concatenate()([x1,x2])\n",
    "\n",
    "\n",
    "B) Average layer :\n",
    "- Layer that averages a list of inputs element-wise\n",
    "- It takes input(list of tensors), all of same shape,Returns single tensor(also of same shape)\n",
    "\n",
    "@ tf.keras.layers.Average(**kwargs)\n",
    "\n",
    ">>> avg=tf.keras.layers.Average()([x1,x2])\n",
    ">>> out=tf.keras.layers.Dense(4)(avg) \n",
    "\n",
    "Exception:\n",
    "ValueError: If there is shape mismatch between inputs & shapes cannot be broadcasted to match\n",
    "\n",
    "\n",
    "C) Maximum layer :\n",
    "- Layer that computes maximum(element-wise) a list of inputs\n",
    "- It takes input(list of tensors),all of same shape,Returns single tensor(also of the same shape)\n",
    "\n",
    "@ tf.keras.layers.Maximum()\n",
    "\n",
    ">>> maxed=tf.keras.layers.Maximum()([x1,x2])\n",
    "\n",
    "\n",
    "D) Minimum layer :\n",
    "- Layer that computes minimum(element-wise) a list of inputs\n",
    "- It takes input(list of tensors),all of same shape,Returns single tensor(also of the same shape)\n",
    "\n",
    "@ tf.keras.layers.Minimum()\n",
    "\n",
    ">>> minned=tf.keras.layers.Minimum()([x1,x2])\n",
    "\n",
    "\n",
    "E) Add layer :\n",
    "- Layer that adds a list of inputs.\n",
    "- It takes input(list of tensors),all of same shape,Returns single tensor(also of the same shape)\n",
    "\n",
    "@ tf.keras.layers.Add()\n",
    "\n",
    ">>> added=tf.keras.layers.Add()([x1,x2])\n",
    ">>> out=tf.keras.layers.Dense(4)(added)\n",
    "\n",
    "\n",
    "\n",
    "F) Subtract layer :\n",
    "- Layer that subtracts two inputs\n",
    "- It takes input(list of tensors of size 2),both of same shape,Returns single tensor(inputs[0]-inputs[1]), also of the same shape\n",
    "\n",
    "@ tf.keras.layers.Subtract()\n",
    "\n",
    ">>> subtracted=keras.layers.Subtract()([x1,x2])\n",
    ">>> out=keras.layers.Dense(4)(subtracted)\n",
    "\n",
    "\n",
    "G) Multiply layer :\n",
    "- Layer that multiplies (element-wise) a list of inputs.\n",
    "- It takes input(list of tensors),all of the same shape,Returns single tensor (also of same shape).\n",
    "\n",
    "@ tf.keras.layers.Multiply()\n",
    "\n",
    ">>> multiplied=tf.keras.layers.Multiply()([x1,x2])\n",
    "\n",
    "H) Dot layer : \n",
    "- Layer that computes dot product between samples in two tensors\n",
    "Ex: if applied to list of two tensors a & b of shape(batch_size,n),output will be tensor of shape (batch_size,1) \n",
    "    where each entry i will be dot product between a[i] & b[i]    \n",
    "\n",
    "@ tf.keras.layers.Dot(axes,normalize=False)\n",
    "\n",
    ">>> dotted=tf.keras.layers.Dot(axes=1)([x1,x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally-connected layers :\n",
    "**A) LocallyConnected1D layer :** Locally-connected layer for 1D inputs\n",
    "\n",
    "https://keras.io/api/layers/locally_connected_layers/locall_connected1d/\n",
    "\n",
    "\n",
    "**B) LocallyConnected2D layer :** Locally-connected layer for 2D inputs\n",
    "\n",
    "https://keras.io/api/layers/locally_connected_layers/locall_connected2d/\n",
    "\n",
    "A) LocallyConnected1D layer : Locally-connected layer for 1D inputs\n",
    "- Works similarly to the Conv1D layer,except that weights are unshared,that is, a different set of filters is applied at each different patch of input\n",
    "Note: layer attributes cannot be modified after layer has been called once (except the trainable attribute)\n",
    "\n",
    "\n",
    "@ tf.keras.layers.LocallyConnected1D(filters,kernel_size,strides=1,padding=\"valid\",data_format=None,activation=None,use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",bias_initializer=\"zeros\",kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,\n",
    "    kernel_constraint=None,bias_constraint=None,implementation=1)\n",
    "\n",
    "* filters:(Int) Filters to be used\n",
    " * kernel_size:(int|tuple|list of 2 int) specifying width & height of 2D convolution window\n",
    " * strides:(int|tuple|list of 2 int) specifying strides of convolution along width & height\n",
    " * padding: same\":save shape after padding  && \"valid\": no padding.\n",
    " * data_format:(str) \"channels_last\":inputs with shape(batch,height,width,channels)   &&  channels_first:inputs with shape(batch,channels,height,width)\n",
    " * activation: Activation function to use\n",
    " * use_bias:(Bool) whether layer uses a bias vector\n",
    " * kernel_initializer,bias_initializer: Initializer for kernel weights , bias\n",
    " * kernel_regularizer,bias_regularizer,activity_regularizer: Regularizer function applied to kernel weights & bias vector & output of layer(\"activation\")\n",
    " * kernel_constraint,bias_constraint: Constraint function applied to kernel & bias \n",
    " * implementation: 1: large & dense models | 2: small models | 3: large & sparse model\n",
    "    1=loops over input spatial locations to perform forward pass(memory-efficient but performs lot of (small) ops)\n",
    "    2=stores layer-weights in dense but sparsely-populated 2D matrix & implements forward pass as single matrix-multiply(Uses lot of RAM but performs few(large)ops)\n",
    "    3=stores layer-weights in sparse tensor & implements forward pass as single sparse matrix-multiply\n",
    "\n",
    ">>> model.add(LocallyConnected1D(32,3))\n",
    "\n",
    "Input: 3D tensor with shape:(batch_size,steps,input_dim)\n",
    "Output: 3D tensor with shape:(batch_size,new_steps,filters) steps value might have changed due to padding or stride\n",
    "\n",
    "\n",
    "B) LocallyConnected2D layer : Locally-connected layer for 2D inputs\n",
    "- Works similarly to 'Conv2D layer',except that weights are unshared,that is, a different set of filters is applied at each different patch of input.\n",
    "Note: layer attributes cannot be modified after layer has been called once(except trainable attribute)\n",
    "\n",
    "@ tf.keras.layers.LocallyConnected2D(filters,kernel_size,strides=(1,1),padding=\"valid\",data_format=None,activation=None,\n",
    "    use_bias=True,kernel_initializer=\"glorot_uniform\",bias_initializer=\"zeros\",kernel_regularizer=None,bias_regularizer=None,\n",
    "    activity_regularizer=None,kernel_constraint=None,bias_constraint=None,implementation=1)\n",
    "\n",
    " * filters:(Int) Filters to be used\n",
    " * kernel_size:(int|tuple|list of 2 int) specifying width & height of 2D convolution window\n",
    " * strides:(int|tuple|list of 2 int) specifying strides of convolution along width & height\n",
    " * padding: same\":save shape after padding  && \"valid\": no padding.\n",
    " * data_format:(str) \"channels_last\":inputs with shape(batch,height,width,channels)   &&  channels_first:inputs with shape(batch,channels,height,width)\n",
    " * activation: Activation function to use\n",
    " * use_bias:(Bool) whether layer uses a bias vector\n",
    " * kernel_initializer,bias_initializer: Initializer for kernel weights , bias\n",
    " * kernel_regularizer,bias_regularizer,activity_regularizer: Regularizer function applied to kernel weights & bias vector & output of layer(\"activation\")\n",
    " * kernel_constraint,bias_constraint: Constraint function applied to kernel & bias \n",
    " * implementation: 1: large & dense models | 2: small models | 3: large & sparse model\n",
    "    1=loops over input spatial locations to perform forward pass(memory-efficient but performs lot of (small) ops)\n",
    "    2=stores layer-weights in dense but sparsely-populated 2D matrix & implements forward pass as single matrix-multiply(Uses lot of RAM but performs few(large)ops)\n",
    "    3=stores layer-weights in sparse tensor & implements forward pass as single sparse matrix-multiply\n",
    "\n",
    ">>> model.add(LocallyConnected2D(32,(3,3)))\n",
    "\n",
    "Input: 4D tensor with shape:(samples,channels,rows,cols) if data_format='channels_first' \n",
    "       4D tensor with shape:(samples,rows,cols,channels) if data_format='channels_last'.\n",
    "\n",
    "Output:4D tensor with shape:(samples,filters,new_rows,new_cols) if data_format='channels_first'\n",
    "       4D tensor with shape:(samples,new_rows,new_cols,filters) if data_format='channels_last'\n",
    "\n",
    "\n",
    "\n",
    "# Attention Layers : \n",
    "A) MultiHeadAttention layer :\n",
    "\n",
    "https://keras.io/api/layers/attention_layers/multi_head_attention/\n",
    "\n",
    "B) Attention layer :\n",
    "\n",
    "https://keras.io/api/layers/attention_layers/attention/\n",
    "\n",
    "C) AdditiveAttention layer :\n",
    "\n",
    "https://keras.io/api/layers/attention_layers/additive_attention/\n",
    "\n",
    "\n",
    "A) MultiHeadAttention layer :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "B) Attention layer :\n",
    "\n",
    "@ tf.keras.layers.Attention(use_scale=False,causal=,dropout=)\n",
    " * use_scale:True, will create scalar variable to scale attention scores.\n",
    " * causal: Boolean. Set to True for decoder self-attention. Adds a mask such that position i cannot attend to positions j > i. This prevents the flow of information from the future towards the past.\n",
    " * dropout: Float between 0 and 1. Fraction of the units to drop for the attention scores.\n",
    "\n",
    "\n",
    "\n",
    "C) AdditiveAttention layer :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Losses :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## A) Probabilistic losses :\n",
    "**1) BinaryCrossentropy():** when 2-label classes(assumed to be 0 & 1) & single floating-point value per prediction\n",
    "***binary_crossentropy():*** Computes binary crossentropy loss\n",
    "       \n",
    "**2) CategoricalCrossentropy():** when two/more label classes & labels provided in one_hot representation\n",
    "***categorical_crossentropy():*** Computes the categorical crossentropy loss\n",
    "\n",
    "**3) SparseCategoricalCrossentropy():** when two|more label classes & expect labels to be provided as integers\n",
    "***sparse_categorical_crossentropy():*** Computes Sparse categorical crossentropy loss\n",
    "\n",
    "**4) Poisson():** Computes Poisson loss between y_true & y_pred\n",
    "***poisson():*** Computes the Poisson loss between y_true & y_pred\n",
    "\n",
    "**5) KLDivergence():** Computes Kullback-Leibler divergence loss between y_true & y_pred\n",
    "***kl_divergence():*** Computes Kullback-Leibler divergence loss between y_true & y_pred\n",
    "\n",
    "https://keras.io/api/losses/probabilistic_losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "                                 A) Probabilistic losses\n",
    "\n",
    "1) BinaryCrossentropy():Computes 'crossentropy loss' between true labels & predicted labels\n",
    " - both 'y_pred' and 'y_true' have shape[batch_size]\n",
    "Used: when 2-label classes(assumed to be 0 & 1) & single floating-point value per prediction\n",
    "\n",
    "@ tf.keras.losses.BinaryCrossentropy(from_logits=False,label_smoothing=0,reduction=\"auto\",name=\"binary_crossentropy\")    \n",
    ">>> model.compile(optimizer,loss=tf.keras.losses.BinaryCrossentropy(),metrics)\n",
    ">>> model.compile(optimizer,loss=\"binary_crossentropy\",metrics)\n",
    "\n",
    "binary_crossentropy() : Computes binary crossentropy loss\n",
    "@ tf.keras.losses.binary_crossentropy(y_true,y_pred,from_logits=False,label_smoothing=0)\n",
    ">>> loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "2) CategoricalCrossentropy():Computes 'crossentropy loss' between labels & predictions\n",
    " - 'y_pred' and 'y_true' have shape[batch_size,num_classes]\n",
    "Used: when two/more label classes & labels provided in one_hot representation\n",
    "    \n",
    "@ tf.keras.losses.CategoricalCrossentropy(from_logits=False,label_smoothing=0,reduction=\"auto\",name=\"categorical_crossentropy\")\n",
    ">>> model.compile(optimizer,loss=tf.keras.losses.CategoricalCrossentropy(),metrics)\n",
    ">>> model.compile(optimizer,loss='CategoricalCrossentropy',metrics)\n",
    "\n",
    "categorical_crossentropy() : Computes the categorical crossentropy loss.\n",
    "@ tf.keras.losses.categorical_crossentropy(y_true,y_pred,from_logits=False,label_smoothing=0)\n",
    ">>> loss=tf.keras.losses.categorical_crossentropy(y_true,y_pred)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "3) SparseCategoricalCrossentropy(): Computes 'crossentropy loss' between labels & predictions\n",
    " - shape of 'y_true' [batch_size] & shape of 'y_pred'[batch_size,num_classes]\n",
    "Used: when two|more label classes & expect labels to be provided as integers\n",
    "\n",
    "@ tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False,reduction=\"auto\",name=\"SparseCategoricalCrossentropy\")    \n",
    ">>> model.compile(optimizer,loss=tf.keras.losses.BinaryCrossentropy(),metrics)\n",
    ">>> model.compile(optimizer,loss='SparseCategoricalCrossentropy',metrics)\n",
    "\n",
    "sparse_categorical_crossentropy(): Computes Sparse categorical crossentropy loss\n",
    "@ tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred,from_logits=False,axis=-1)\n",
    ">>> loss=tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "4) Poisson(): Computes Poisson loss between y_true and y_pred.\n",
    "  loss=y_pred-y_true*log(y_pred)\n",
    "\n",
    "@ tf.keras.losses.Poisson(reduction=\"auto\",name=\"poisson\")    \n",
    ">>> model.compile(optimizer,loss=tf.keras.losses.Poisson(),metrics)\n",
    ">>> model.compile(optimizer,loss='poisson',metrics)\n",
    "\n",
    "poisson(): Computes the Poisson loss between y_true & y_pred\n",
    " - Poisson loss is mean of elements of Tensor y_pred-y_true*log(y_pred)\n",
    "@ tf.keras.losses.poisson(y_true,y_pred)\n",
    ">>> loss=tf.keras.losses.poisson(y_true,y_pred)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "5) KLDivergence(): Computes Kullback-Leibler divergence loss between y_true & y_pred\n",
    " loss=y_true*log(y_true/y_pred)\n",
    "\n",
    "@ tf.keras.losses.KLDivergence(reduction=\"auto\", name=\"kl_divergence\")    \n",
    ">>> model.compile(optimizer,loss=tf.keras.losses.Poisson(),metrics)\n",
    ">>> model.compile(optimizer,loss='kl_divergence',metrics)\n",
    "\n",
    "kl_divergence(): Computes Kullback-Leibler divergence loss between y_true & y_pred.\n",
    "loss=y_true*log(y_true/y_pred)\n",
    "@ tf.keras.losses.kl_divergence(y_true, y_pred)\n",
    ">>> loss=tf.keras.losses.kullback_leibler_divergence(y_true,y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## B) Regression losses :\n",
    "**1) MeanSquaredError():** Computes \"mean of squares of errors\" between labels & predictions\n",
    "\n",
    "**2) MeanAbsoluteError():** Computes \"mean of absolute difference\" between labels & predictions\n",
    "\n",
    "**3) MeanAbsolutePercentageError():** Computes \"mean absolute percentage error\" between y_true & y_pred\n",
    "\n",
    "**4) MeanSquaredLogarithmicError():** Computes \"mean squared logarithmic error\" between y_true & y_pred\n",
    "\n",
    "**5) CosineSimilarity():** Computes \"cosine similarity\" between labels & predictions\n",
    "\n",
    "**6) Huber():** Computes \"Huber loss\" between y_true & y_pred\n",
    "\n",
    "**7) LogCosh():** Computes logarithm of 'hyperbolic cosine' of  prediction error.\n",
    "\n",
    "https://keras.io/api/losses/regression_losses/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "                                       B) Regression losses\n",
    "\n",
    "1) MeanSquaredError(): Computes \"mean of squares of errors\" between labels & predictions\n",
    "   loss=square(y_true-y_pred)\n",
    "\n",
    "@ tf.keras.losses.MeanSquaredError(reduction=\"auto\",name=\"mean_squared_error\")\n",
    "model.compile(optimizer='sgd',loss=tf.keras.losses.MeanSquaredError(),metrics)\n",
    "model.compile(optimizer='sgd',loss='mean_squared_error',metrics)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "2) MeanAbsoluteError(): Computes \"mean of absolute difference\" between labels & predictions\n",
    "   loss=abs(y_true-y_pred)\n",
    "\n",
    "@ tf.keras.losses.MeanSquaredError(reduction=\"auto\",name=\"mean_absolute_error\")\n",
    "model.compile(optimizer='sgd',loss=tf.keras.losses.MeanAbsoluteError(),metrics)\n",
    "model.compile(optimizer='sgd',loss='mean_squared_error',metrics)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "3) MeanAbsolutePercentageError(): Computes \"mean absolute percentage error\" between y_true & y_pred\n",
    "   loss=100*abs(y_true-y_pred)/y_true\n",
    "\n",
    "@ tf.keras.losses.MeanAbsolutePercentageError(reduction=\"auto\",name=\"mean_absolute_percentage_error\")\n",
    "model.compile(optimizer='sgd',loss=tf.keras.losses.MeanAbsolutePercentageError(),metrics)\n",
    "model.compile(optimizer='sgd',loss='mean_absolute_percentage_error',metrics)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "4) MeanSquaredLogarithmicError(): Computes \"mean squared logarithmic error\" between y_true & y_pred\n",
    "   loss=square(log(y_true+1.)-log(y_pred+1.))\n",
    "\n",
    "@ tf.keras.losses.MeanSquaredLogarithmicError(reduction=\"auto\",name=\"mean_squared_logarithmic_error\")\n",
    "model.compile(optimizer='sgd',loss=tf.keras.losses.MeanSquaredLogarithmicError(),metrics)\n",
    "model.compile(optimizer='sgd',loss='mean_squared_logarithmic_error',metrics)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "5) CosineSimilarity() : Computes \"cosine similarity\" between labels & predictions\n",
    "   loss=-sum(l2_norm(y_true)*l2_norm(y_pred))\n",
    "\n",
    "@ tf.keras.losses.CosineSimilarity(axis=-1,reduction=\"auto\",name=\"cosine_similarity\")\n",
    "model.compile(optimizer='sgd',loss=tf.keras.losses.CosineSimilarity(),metrics)\n",
    "model.compile(optimizer='sgd',loss='cosine_similarity',metrics)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "6) Huber(): Computes \"Huber loss\" between y_true & y_pred\n",
    " - For each x in error = y_true - y_pred\n",
    "\n",
    "@ tf.keras.losses.Huber(delta=1.0,reduction=\"auto\",name=\"huber_loss\")\n",
    "model.compile(optimizer='sgd',loss=tf.keras.losses.Huber(),metrics)\n",
    "model.compile(optimizer='sgd',loss='huber_loss',metrics)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "7) LogCosh(): Computes logarithm of 'hyperbolic cosine' of  prediction error.\n",
    "   logcosh=log((exp(x)+exp(-x))/2)     where x=error y_pred-y_true\n",
    "\n",
    "@ tf.keras.losses.LogCosh(reduction=\"auto\",name=\"log_cosh\")\n",
    "model.compile(optimizer='sgd',loss=LogCosh(),metrics)\n",
    "model.compile(optimizer='sgd',loss='log_cosh',metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## C) Hinge losses : \n",
    "**1) Hinge():** Computes \"hinge loss\" between y_true & y_pred\n",
    "\n",
    "**2) SquaredHinge():** Computes \"squared hinge loss\" between y_true & y_pred.\n",
    "\n",
    "**3) CategoricalHinge():** Computes \"categorical hinge loss\" between y_true & y_pred\n",
    "\n",
    "https://keras.io/api/losses/hinge_losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "                                    C) Hinge losses: For \"maximum-margin\" classification\n",
    "\n",
    "1) Hinge() : Computes \"hinge loss\" between y_true & y_pred.\n",
    "     loss=maximum(1-y_true*y_pred,0)\n",
    "-'y_true' values expected to be -1|1.\n",
    "   If binary (0|1) labels are provided,convert them to -1|1\n",
    " \n",
    "@ tf.keras.losses.Hinge(reduction=\"auto\",name=\"hinge\")\n",
    "model.compile(optimizer='sgd',loss=tf.keras.losses.Hinge(),metrics)\n",
    "model.compile(optimizer='sgd',loss='hinge',metrics)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "2) SquaredHinge(): Computes \"squared hinge loss\" between y_true & y_pred.\n",
    "   loss=square(maximum(1-y_true*y_pred,0))\n",
    "- 'y_true' values expected to be -1|1\n",
    "   If binary (0|1) labels are provided we will convert them to -1|1\n",
    "\n",
    "@ tf.keras.losses.SquaredHinge(reduction=\"auto\", name=\"squared_hinge\")\n",
    "model.compile(optimizer='sgd',loss=tf.keras.losses.SquaredHinge(),metrics)\n",
    "model.compile(optimizer='sgd',loss='squared_hinge',metrics)\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "3) CategoricalHinge(): Computes \"categorical hinge loss\" between y_true & y_pred\n",
    " loss=maximum(neg-pos+1,0)      where neg=maximum((1-y_true)*y_pred) & pos=sum(y_true*y_pred)\n",
    "\n",
    "@ tf.keras.losses.CategoricalHinge(reduction=\"auto\",name=\"categorical_hinge\")\n",
    "model.compile(optimizer='sgd',loss=tf.keras.losses.CategoricalHinge(),metrics)\n",
    "model.compile(optimizer='sgd',loss='categorical_hinge',metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Metrics :\n",
    "\n",
    "**2.1) Accuracy metrics :**\n",
    "\n",
    "**2.2) Probabilistic metrics :**\n",
    "\n",
    "**2.3) Regression metrics :**\n",
    "\n",
    "**2.4) Classification metrics :**\n",
    "\n",
    "**2.5) Image segmentation metrics :**\n",
    "\n",
    "**2.6) Hinge metrics  :**\n",
    "\n",
    "**2.7) Creating custom metrics :**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##  Accuracy metrics :\n",
    "**a) Accuracy():** Calculated, how often predictions are equal to labels \n",
    "\n",
    "**b) BinaryAccuracy():** Calculates how often predictions match binary labels\n",
    "\n",
    "**c) Categorical():** Calculates how often predictions matches one-hot labels\n",
    "\n",
    "**d) TopKCategoricalAccuracy():** Computes how often targets are in the top K predictions\n",
    "\n",
    "**e) SparseTopKCategoricalAccuracy():** Computes how often integer targets are in top K predictions\n",
    "https://keras.io/api/metrics/accuracy_metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a) Accuracy(): Calculated, how often predictions are equal to labels \n",
    "\n",
    "@ tf.keras.metrics.Accuracy(name=\"accuracy\",dtype=None)\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.Accuracy()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "b) BinaryAccuracy(): Calculates how often predictions match binary labels\n",
    "\n",
    "@ tf.keras.metrics.tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\",dtype=None,threshold=0.5)\n",
    " ** threshold:(Float)threshold for whether prediction values are 1 or 0\n",
    "\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "c) Categorical(): Calculates how often predictions matches one-hot labels\n",
    "@ tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\",dtype=None)\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.Categorical()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "d) TopKCategoricalAccuracy(): Computes how often targets are in the top K predictions\n",
    "@ tf.keras.metrics.TopKCategoricalAccuracy(k=5,name=\"top_k_categorical_accuracy\",dtype=None)\n",
    " ** k: No. of top elements to look at for computing accuracy(Defaults 5)\n",
    "\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.TopKCategoricalAccuracy()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "e) SparseTopKCategoricalAccuracy(): Computes how often integer targets are in top K predictions\n",
    "@ tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5,name=\"sparse_top_k_categorical_accuracy\",dtype=None)\n",
    " ** k: No. of top elements to look at for computing accuracy(Defaults 5)\n",
    "\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Probabilistic metrics :\n",
    "**a) BinaryCrossentropy():** when there are only two label classes (0 & 1)\n",
    "\n",
    "**b) CategoricalCrossentropy():** when there multiple label classes(2 or more)\n",
    "\n",
    "**c) SparseCategoricalCrossentropy():** When there are two|more label classes(expect labels to be provided as int)\n",
    "\n",
    "**d) KLDivergence():** Computes 'Kullback-Leibler divergence metric' between y_true & y_pred\n",
    "\n",
    "**e) Poisson():** Computes 'poisson metric' between y_true & y_pred\n",
    "\n",
    "https://keras.io/api/metrics/probabilistic_metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a) BinaryCrossentropy(): Computes 'crossentropy metric' between the labels & predictions\n",
    "Used: when there are only two label classes (0 & 1)\n",
    "\n",
    "@ tf.keras.metrics.BinaryCrossentropy(name=\"binary_crossentropy\",dtype=None,from_logits=False,label_smoothing=0)\n",
    "    ** from_logits:Whether o/p expected to be logits tensor(By default:we consider,o/p encodes probability distribution)\n",
    "    ** label_smoothing:(Float [0,1]) When > 0:label values are smoothed, meaning the confidence on label values are relaxed\n",
    "\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.BinaryCrossentropy()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "b) CategoricalCrossentropy(): Computes 'crossentropy metric' between labels & predictions\n",
    "Used: when there multiple label classes(2 or more)\n",
    "\n",
    "@ tf.keras.metrics.CategoricalCrossentropy(name=\"categoricalcrossentropy\",dtype=None,from_logits=False,label_smoothing=0)\n",
    "    ** from_logits:Whether o/p expected to be logits tensor(By default:we consider,o/p encodes probability distribution)\n",
    "    ** label_smoothing:(Float [0,1]) When > 0:label values are smoothed, meaning the confidence on label values are relaxed\n",
    "\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.CategoricalCrossentropy()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "c) SparseCategoricalCrossentropy(): Computes 'crossentropy metric' between the labels & predictions\n",
    "Used: When there are two|more label classes(expect labels to be provided as int)\n",
    "\n",
    "@ tf.keras.metrics.SparseCategoricalCrossentropy(name=\"sparseccrossentropy\",dtype=None,from_logits=False,label_smoothing=0)\n",
    "    ** from_logits:Whether o/p expected to be logits tensor(By default:we consider,o/p encodes probability distribution)\n",
    "    ** axis:dimension along which the metric is computed(Defaults -1)\n",
    "\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.SparseCategoricalCrossentropy()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "d) KLDivergence(): Computes 'Kullback-Leibler divergence metric' between y_true & y_pred\n",
    "   metric=y_true*log(y_true/y_pred)   \n",
    "\n",
    "@ tf.keras.metrics.KLDivergence(name=\"kullback_leibler_divergence\",dtype=None)\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.KLDivergence()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "e) Poisson(): Computes 'poisson metric' between y_true & y_pred\n",
    " metric=y_pred-y_true*log(y_pred)  \n",
    "\n",
    "@ tf.keras.metrics.Poisson(name=\"poisson\",dtype=None)\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.Poisson()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Regression metrics :\n",
    "**a) MeanSquaredError():** Computes 'mean squared error' between y_true & y_pred\n",
    "\n",
    "**b) RootMeanSquaredError():** Computes 'root mean squared error metric' between y_true & y_pred\n",
    "\n",
    "**c) MeanAbsoluteError():** Computes 'mean absolute error' between labels & predictions\n",
    "\n",
    "**d) MeanAbsolutePercentageError():** Computes 'mean absolute percentage error' between y_true & y_pred\n",
    "\n",
    "**e) MeanSquaredLogarithmicError():** Computes 'mean squared logarithmic error' between y_true & y_pred\n",
    "\n",
    "**f) CosineSimilarity():** Computes 'cosine similarity' between labels & predictions\n",
    "\n",
    "**g) LogCoshError():** Computes 'logarithm of hyperbolic cosine' of prediction error\n",
    "\n",
    "https://keras.io/api/metrics/regression_metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a) MeanSquaredError(): Computes 'mean squared error' between y_true & y_pred\n",
    "\n",
    "@ tf.keras.metrics.MeanSquaredError(name=\"mean_squared_error\",dtype=None)\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "b) RootMeanSquaredError(): Computes 'root mean squared error metric' between y_true & y_pred\n",
    "\n",
    "@ tf.keras.metrics.RootMeanSquaredError(name=\"root_mean_squared_error\",dtype=None)\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "c) MeanAbsoluteError(): Computes 'mean absolute error' between labels & predictions\n",
    "@ tf.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\",dtype=None)\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "d) MeanAbsolutePercentageError(): Computes 'mean absolute percentage error' between y_true & y_pred\n",
    "\n",
    "@ tf.keras.metrics.MeanAbsolutePercentageError(name=\"mean_absolute_percentage_error\",dtype=None)\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "e) MeanSquaredLogarithmicError(): Computes 'mean squared logarithmic error' between y_true & y_pred\n",
    "\n",
    "@ tf.keras.metrics.MeanSquaredLogarithmicError(name=\"mean_squared_logarithmic_error\",dtype=None)\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "f) CosineSimilarity(): Computes 'cosine similarity' between labels & predictions\n",
    " cosine_similarity=(a.b)/||a|| ||b||\n",
    "\n",
    "@ tf.keras.metrics.CosineSimilarity(name=\"cosine_similarity\",dtype=None,axis=-1)\n",
    "** axis: dimension along which the cosine similarity is computed(Defaults to -1)\n",
    "\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.CosineSimilarity()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "g) LogCoshError(): Computes 'logarithm of hyperbolic cosine' of prediction error\n",
    "   logcosh=log((exp(x)+exp(-x))/2),          where x= error (y_pred - y_true)\n",
    "\n",
    "@ tf.keras.metrics.LogCoshError(name=\"logcosh\",dtype=None)\n",
    "** axis: dimension along which the cosine similarity is computed(Defaults to -1)\n",
    "\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.LogCoshError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Classification metrics based on True/False positives & negatives\n",
    "**a) AUC():** Computes approximate AUC (Area under curve) via Riemann sum\n",
    "\n",
    "**b) Precision():** Computes precision of predictions with respect to labels\n",
    "\n",
    "**c) Recall():** Computes recall of predictions with respect to labels \n",
    "\n",
    "**d) TruePositives():** Calculates number of true positives\n",
    "\n",
    "**e) TrueNegatives():** Calculates number of true -ve\n",
    "\n",
    "**f) FalsePositives():** Calculates number of false -ve\n",
    "\n",
    "**g) FalseNegatives():** Calculates number of false +ve\n",
    "\n",
    "**h) PrecisionAtRecall():** Computes best precision where(recall  >= specified value)\n",
    "\n",
    "**i) SensitivityAtSpecificity():** Computes best sensitivity where (specificity  >= specified value)\n",
    "\n",
    "**j) SpecificityAtSensitivity():** Computes best specificity where (sensitivity >= specified value)\n",
    "\n",
    "https://keras.io/api/metrics/classification_metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a) AUC():Computes approximate AUC (Area under curve) via Riemann sum\n",
    "- Metric creates four local_variables, 'tp','tn','fp','fn' that used to compute AUC\n",
    "- Best results,predictions should be distributed approximately uniformly in range[0,1] & not peaked around 0|1\n",
    "\n",
    "@ tf.keras.metrics.AUC(num_thresholds=200,curve=\"ROC\",summation_method=\"interpolation\",name=None,dtype=None,thresholds=None,multi_label=False,label_weights=None)  \n",
    "    ** num_thresholds:No. of thresholds to use when discretizing roc curve(Values must be > 1)\n",
    "    ** curve:Specifies name of curve to be computed,('PR' for Precision-Recall-curve)\n",
    "    ** summation_method:Specifies 'Riemann summation' method used\n",
    "             'interpolation': applies mid-point summation scheme for ROC\n",
    "              PR-AUC, interpolates (true/false) positives but not the ratio that is precision\n",
    "             'minoring': applies left summation for increasing intervals & right summation for decreasing intervals\n",
    "             'majoring' does opposite of minor\n",
    "    ** thresholds:(list of float)use as thresholds for discretizing curve(Values should be[0,1]) \n",
    "    * multi_label:(bool) indicating whether multilabel data should be treated as such, wherein AUC is computed separately for each label and then averaged across labels,\n",
    "        (when False) if the data should be flattened into a single label before AUC computation. In the latter case, when multilabel data is passed to AUC, each label-prediction pair is treated as an individual data point. Should be set to False for multi-class data.\n",
    "    ** label_weights:(list|array|tensor of non-ve) weights used to compute AUCs for multilabel data\n",
    "           When multi_label:True,weights are applied to individual label AUCs when they are averaged to produce multi-label AUC\n",
    "                           :False,they used to weight individual label predictions in computing confusion matrix on flattened data\n",
    "\n",
    ">>> model.compile(optimizer='sgd',loss='mse',metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "b) Precision(): Computes precision of predictions with respect to labels\n",
    "- Metric creates two local_variables 'tp','fp',used to compute precision\n",
    "- These returned as 'precision',by calculating (TP+FP)/TP\n",
    "\n",
    "@ tf.keras.metrics.Precision(thresholds=None,top_k=None,class_id=None,name=None,dtype=None)\n",
    "    * thresholds:(list of float)use as thresholds for discretizing curve(Values should be[0,1])\n",
    "        threshold is compared with prediction values to determine the truth value of predictions(above threshold is true,below is false)\n",
    "    * multi_label:(bool)indicating whether multilabel data should b threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false)\n",
    "    ** top_k:(int)specify top-k predictions to consider when calculating precision(Unset by default)\n",
    "    ** class_id:(int)class ID for which we want binary metrics(must be in half-open interval[0,num_classes), where num_classes=last dimension of predictions)\n",
    "   \n",
    ">>> model.compile(optimizer='sgd',loss='mse',metrics=[tf.keras.metrics.Precision()])\n",
    "                                                                                                                  \n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "c) Recall(): Computes recall of predictions with respect to labels                                                                                     \n",
    "- metric creates two local var,'true_positives' & 'false_negatives',used to compute recall\n",
    "recall = (TP + FN)/TP\n",
    "\n",
    "@ tf.keras.metrics.Recall(thresholds=None,top_k=None,class_id=None,name=None,dtype=None)\n",
    "    * thresholds:(list of float)use as thresholds for discretizing curve(Values should be[0,1])\n",
    "        threshold is compared with prediction values to determine the truth value of predictions(above threshold is true,below is false)\n",
    "    ** top_k:(int)specify top-k predictions to consider when calculating precision(Unset by default)\n",
    "    ** class_id:(int)class ID for which we want binary metrics(must be in half-open interval[0,num_classes), where num_classes=last dimension of predictions)\n",
    "   \n",
    ">>> model.compile(optimizer='sgd',loss='mse',metrics=[tf.keras.metrics.Recall()])                                                                                         \n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "d) TruePositives(): Calculates number of true positives\n",
    "- If 'sample_weight'given,calculates sum of weights of 'true positives'.creates 'true_positives'local var that keep track of number of 'true positives'                                                                                                                                                                               \n",
    "\n",
    "@ tf.keras.metrics.TruePositives(thresholds=0.5,name=None,dtype=None)\n",
    "    ** thresholds:(float|list|tuple of float)threshold values in [0,1]\n",
    "        threshold,compared with prediction values to determine 'truth value' of predictions(above threshold is true,below is false)\n",
    "        One metric value is generated for each threshold value\n",
    "    \n",
    "model.compile(optimizer='sgd',loss='mse',metrics=[tf.keras.metrics.TruePositives()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "e) TrueNegatives(): Calculates number of true -ve\n",
    "- If 'sample_weight'given,calculates sum of weights of 'true -ve'.creates 'accumulator'local var that keep track of number of 'true -ve'                                                                                                                                                                               \n",
    "\n",
    "@ tf.keras.metrics.TrueNegatives(thresholds=0.5,name=None,dtype=None)\n",
    "    ** thresholds:(float|list|tuple of float)threshold values in [0,1]\n",
    "        threshold,compared with prediction values to determine 'truth value' of predictions(above threshold is true,below is false)\n",
    "        One metric value is generated for each threshold value\n",
    "    \n",
    ">>> model.compile(optimizer='sgd',loss='mse',metrics=[tf.keras.metrics.TrueNegatives()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "f) FalsePositives():Calculates number of false -ve\n",
    "- If 'sample_weight'given,calculates sum of weights of 'false +ve'.creates 'accumulator'local var that keep track of number of 'false +ve'                                                                                                                                                                               \n",
    "\n",
    "@ tf.keras.metrics.FalsePositives(thresholds=0.5,name=None,dtype=None)\n",
    "    ** thresholds:(float|list|tuple of float)threshold values in [0,1]\n",
    "        threshold,compared with prediction values to determine 'truth value' of predictions(above threshold is true,below is false)\n",
    "        One metric value is generated for each threshold value\n",
    "\n",
    ">>> model.compile(optimizer='sgd',loss='mse',metrics=[tf.keras.metrics.FalsePositives()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "g) FalseNegatives(): Calculates number of false +ve\n",
    "- If 'sample_weight'given,calculates sum of weights of 'false -ve'.creates 'accumulator'local var that keep track of number of 'false -ve'                                                                                                                                                                               \n",
    "\n",
    "@ tf.keras.metrics.FalsePositives(thresholds=0.5,name=None,dtype=None)\n",
    "    ** thresholds:(float|list|tuple of float)threshold values in [0,1]\n",
    "        threshold,compared with prediction values to determine 'truth value' of predictions(above threshold is true,below is false)\n",
    "        One metric value is generated for each threshold value\n",
    "\n",
    ">>> model.compile(optimizer='sgd',loss='mse',metrics=[tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "h) PrecisionAtRecall(): Computes best precision where(recall  >= specified value)\n",
    "- metric creates 'tp','tn','fp','fn' four local var which used to compute precision from recall \n",
    "- threshold for 'recall' value computed & evaluate corresponding precision                                                                                        \n",
    "@ tf.keras.metrics.PrecisionAtRecall(recall,num_thresholds=200,name=None,dtype=None)                                                                                         \n",
    "* recall:scalar value in range[0,1]\n",
    "** num_thresholds:No. of thresholds to use for matching given recall\n",
    "\n",
    ">>> model.compile(optimizer='sgd',loss='mse',metrics=[tf.keras.metrics.PrecisionAtRecall(recall=0.8)])\n",
    "                                                                                                                                                                                                                                                                        \n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "i) SensitivityAtSpecificity(): Computes best sensitivity where (specificity  >= specified value)\n",
    "- Sensitivity measures 'actual +ve'that correctly identified(tp/(tp+fn))\n",
    "- Specificity measures 'actual -ve'that are correctly identified(tn/(tn+fp))                                                                                       \n",
    "- metric creates 'tp','tn','fp','fn' four local var,used to compute sensitivity from specificity\n",
    "- threshold for given 'specificity'computed & evaluate corresponding sensitivity\n",
    "\n",
    "@ tf.keras.metrics.SensitivityAtSpecificity(specificity,num_thresholds=200,name=None,dtype=None)\n",
    "* specificity:scalar value in range[0,1]\n",
    "** num_thresholds:No. of thresholds to use for matching given recall\n",
    "\n",
    ">>> model.compile(optimizer='sgd',loss='mse',metrics=[tf.keras.metrics.SensitivityAtSpecificity()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "j) SpecificityAtSensitivity(): Computes best specificity where (sensitivity >= specified value)\n",
    " - Sensitivity measures 'actual +ve'that correctly identified(tp/(tp+fn))\n",
    " - Specificity measures 'actual -ve'that are correctly identified(tn/(tn+fp))                                                                                       \n",
    " - metric creates 'true_positives','true_negatives','false_positives','false_negatives'four local var,used to compute specificity from sensitivity\n",
    " - threshold for given 'specificity'computed & evaluate corresponding specificity\n",
    "\n",
    "@ tf.keras.metrics.SensitivityAtSpecificity(specificity,num_thresholds=200,name=None,dtype=None)\n",
    "    * sensitivity:scalar value in range[0,1]\n",
    "    ** num_thresholds:No. of thresholds to use for matching given recall\n",
    "\n",
    ">>> model.compile(optimizer='sgd',loss='mse',metrics=[tf.keras.metrics.SpecificityAtSensitivity()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Image segmentation metrics :\n",
    "\n",
    "**a) MeanIoU():** Computes \"mean Intersection-Over-Union metric\"\n",
    "\n",
    "https://keras.io/api/metrics/segmentation_metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a) MeanIoU(): Computes \"mean Intersection-Over-Union metric\"\n",
    " - first computes the IOU for each semantic class and then computes the average over classes\n",
    "IOU = true_positive / (true_positive + false_positive + false_negative)\n",
    " - predictions are accumulated in a confusion matrix, weighted by sample_weight and the metric is then calculated from it\n",
    " - If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values\n",
    "\n",
    "@ tf.keras.metrics.MeanIoU(num_classes,name=None,dtype=None)\n",
    "    * num_classes:possible no. of label prediction task can have value must be provided,since confusion matrix of dimension =[num_classes,num_classes] will be allocated\n",
    "\n",
    ">>> model.compile(optimizer,loss,metrics=[tf.keras.metrics.MeanIoU()])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Hinge metrics for \"maximum-margin\" classification\n",
    "**a) Hinge():** Computes hinge metric between y_true & y_pred.\n",
    " \n",
    "**b) SquaredHinge():** Computes 'squared hinge metric' between y_true & y_pred.\n",
    "\n",
    "**c) CategoricalHinge():** Computes 'categorical hinge metric' between y_true & y_pred\n",
    "\n",
    "https://keras.io/api/metrics/hinge_metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a) Hinge(): Computes hinge metric between y_true & y_pred.\n",
    " - y_true values are expected to be -1 or 1\n",
    " - If binary (0 or 1) labels are provided we will convert them to -1 or 1\n",
    "\n",
    "@ tf.keras.metrics.Hinge(name=\"hinge\",dtype=None)\n",
    "model.compile(optimizer,loss,metrics=[tf.keras.metrics.Hinge()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "b) SquaredHinge(): Computes 'squared hinge metric' between y_true & y_pred.\n",
    " - y_true values are expected to be -1 or 1. \n",
    " - If binary (0 or 1) labels are provided we will convert them to -1 or 1\n",
    "\n",
    "@ tf.keras.metrics.SquaredHinge(name=\"hinge\",dtype=None)\n",
    "model.compile(optimizer,loss,metrics=[tf.keras.metrics.SquaredHinge()])\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "c) CategoricalHinge(): Computes 'categorical hinge metric' between y_true & y_pred\n",
    "\n",
    "@ tf.keras.metrics.CategoricalHinge(name=\"categorical_hinge\",dtype=None)\n",
    "model.compile(optimizer,loss,metrics=[tf.keras.metrics.CategoricalHinge()])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers :\n",
    "1) Adam : Optimizer that implements 'Adam algorithm'\n",
    "\n",
    "https://keras.io/api/optimizers/adam/\n",
    "\n",
    "\n",
    "2) SGD :\n",
    "\n",
    "3) RMSprop :\n",
    "\n",
    "\n",
    "Adadelta:\n",
    "https://keras.io/api/optimizers/adadelta/\n",
    "\n",
    "Adagrad: Optimizer that implements the 'Adagrad algorithm'\n",
    "https://keras.io/api/optimizers/adagrad/\n",
    "\n",
    "Adamax\n",
    "\n",
    "Nadam:\n",
    "https://keras.io/api/optimizers/Nadam/\n",
    "\n",
    "Ftrl:\n",
    "https://keras.io/api/optimizers/ftrl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1) Adam : Optimizer that implements 'Adam algorithm'\n",
    "- it is a 'stochastic gradient descent'method, based on adaptive estimation of first-order & second-order moments\n",
    "\n",
    "Computationally efficient(less memory requirement)\n",
    "Invariant to diagonal rescaling of gradients\n",
    "\n",
    "Well for: problems that large in terms of data|parameters\n",
    "    \n",
    "@ tf.keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False,name=\"Adam\")\n",
    " * learning_rate: Returns actual value to use 'learning rate'\n",
    " * beta_1:Returns actual value to use 'exponential decay rate' for 1st moment estimates\n",
    " * beta_2:Returns actual value to use,'exponential decay rate' for 2nd moment estimates\n",
    " * epsilon:Small constant for 'numerical stability'\n",
    " * amsgrad:(Bool) Whether to apply 'AMSGrad variant'\n",
    " ** name:Name for operations created when applying gradients\n",
    "    \n",
    "    \n",
    "\n",
    "2) SGD :\n",
    "\n",
    "\n",
    "\n",
    "3) RMSprop :\n",
    "\n",
    "\n",
    "\n",
    "4) Adagrad : Optimizer that implements the 'Adagrad algorithm'\n",
    "- Optimizer with parameter-specific learning rates,which adapted relative to how frequently parameter gets updated during training\n",
    "- More updates parameter receives, smaller the updates   \n",
    "\n",
    "@ tf.keras.optimizers.Adagrad(learning_rate=0.001,initial_accumulator_value=0.1,epsilon=1e-07,name=\"Adagrad\")\n",
    " * learning_rate: Returns actual value to use 'learning rate'\n",
    " * initial_accumulator_value: Starting value for the accumulators (must be non-negative)\n",
    " * epsilon: Small floating point value to avoid zero denominator\n",
    " * name:Name for operations created when applying gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks API:\n",
    "**Base Callback() :** Abstract base class used to build new callbacks\n",
    "\n",
    "https://www.machinecurve.com/index.php/2020/11/10/an-introduction-to-tensorflow-keras-callbacks/\n",
    "\n",
    "Base Callback('tf.keras.callbacks.Callback()')\n",
    "    * params:(Dict)Training parameters(ex: verbosity,batch size,no. of epochs)\n",
    "    * model: Instance of keras.models.Model. Reference of the model being trained.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ModelCheckpoint() :\n",
    "- Automatically **save model** after each epoch or after specific batch(best one)\n",
    "https://keras.io/api/callbacks/model_checkpoint/\n",
    "\n",
    "\n",
    "ModelCheckpoint() :\n",
    "    \n",
    "@ ModelCheckpoint(filepath,monitor=\"val_loss\",verbose=0,save_best_only=False,\n",
    "    save_weights_only=False,mode=\"auto\",save_freq=\"epoch\",options=None)\n",
    " * monitor:Metric to monitor\n",
    "    -Prefix-name with \"val_\" to monitor validation metrics('val_loss')\n",
    "    -Use \"loss\"|\"val_loss\" to monitor model total loss.\n",
    "    -If specify metrics as strings (ex:\"accuracy\")\n",
    " * mode:{\"min\",\"max\",\"auto\"}\n",
    " * save_freq='epoch':callback saves model after each epoch\n",
    "            =int:callback saves model at end of that many batches  \n",
    "            \n",
    ">>> check_point=tf.keras.callbacks.ModelCheckpoint(filepath=r\"C:\\tp\\\",monitor=\"loss\",verbose=2,save_best_only=True,\n",
    "                       mode=\"auto\",save_freq=\"epoch\")\n",
    ">>> history=model.fit(x,epochs,batch_size,callbacks=[check_point],verbose)\n",
    "\n",
    "\n",
    "## Monitoring  & Visualizing\n",
    "**A|1) TensorBoard():** Monitor training process in realtime\n",
    " - Callback logs for TensorBoard,including:\n",
    "    - **Metrics summary plots**  \n",
    "    - **Training graph visualization**\n",
    "    - **Activation histograms**  \n",
    "    - **Sampled profiling**\n",
    "https://keras.io/api/callbacks/tensorboard/\n",
    "\n",
    "**A|2) RemoteMonitor:** Stream **training process** to server(Requires requests library)\n",
    "- Events sent to root +'/publish/epoch/end/' by default\n",
    "- Calls are HTTP POST,with data argument which is 'JSON-encoded' dictionary of event data\n",
    "\n",
    "https://keras.io/api/callbacks/remote_monitor/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1) TensorBoard() : \n",
    "\n",
    "@ tf.keras.callbacks.TensorBoard(log_dir=\"logs\",histogram_freq=0,write_graph=True,write_images=False,update_freq=\"epoch\",\n",
    "        profile_batch=2,embeddings_freq=0,embeddings_metadata=None) \n",
    " * log_dir:path of directory (where to save log files)\n",
    " * histogram_freq:frequency(in epochs)(to compute activation & weight histograms for layers)\n",
    "           =0: histograms not be computed\n",
    " * write_graph:whether to visualize graph in TensorBoard\n",
    "             =True: log file can become quite large \n",
    " * write_images: whether to write model weights to visualize as image in TensorBoard.\n",
    " * update_freq='batch':writes losses & metrics to TensorBoard after each batch \n",
    "             ='epoch':writes losses & metrics to TensorBoard after each batch\n",
    "             =(int) : writes losses & metrics to TensorBoard after each 'int'batch\n",
    " * profile_batch:Profile batch to sample compute characteristics(must non-ve int|tuple of int)\n",
    " * embeddings_freq: frequency(in epochs) at which embedding layers will be visualized \n",
    "        =0:embeddings not be visualized\n",
    " * embeddings_metadata:(dict)which maps layer name to file name in which metadata for this embedding layer is saved\n",
    "    In case if the same metadata file is used for all embedding layers, string can be passed\n",
    "\n",
    ">>> tb=tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    ">>> history=model.fit(x,epochs,batch_size,callbacks=[tb],verbose)\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------    \n",
    "2) RemoteMonitor() :\n",
    "      \n",
    "@ tf.keras.callbacks.RemoteMonitor(root=\"http://localhost:9000\",path=\"/publish/epoch/end/\",field=\"data\",headers=None,send_as_json=False)        \n",
    " * root:(str)root url of target server.\n",
    " * path:(str)path relative to root to which events will be sent.\n",
    " * field:(str)JSON field under which data will be stored\n",
    "        field is used only if payload is sent within form(i.e. send_as_json=False)\n",
    " ** headers:(Dict.)custom HTTP headers.\n",
    " * send_as_json:(bool) whether request should be sent as \"application/json\"\n",
    "\n",
    ">>> r_monitor=tf.keras.callbacks.RemoteMonitor()\n",
    ">>> history=model.fit(x,steps_per_epoch,epochs,verbose,callbacks=[r_monitor])\n",
    "    \n",
    "URL: \"http://localhost:9000\"\n",
    "\n",
    "\n",
    "## Writing to Log-File\n",
    "**1) CSVLogger :** Writes **log-file** to CSV file(contains Epochs,Accuracy,Loss)\n",
    "\n",
    "**2) ProgbarLogger :** Determine what to print in **keras progress bar**              \n",
    "\n",
    "\n",
    "\n",
    "1) CSVLogger():\n",
    "    \n",
    "@ tf.keras.callbacks.CSVLogger(filename,separator=\",\",append=False)\n",
    " * filename:Filename of the CSV file\n",
    " * separator:String used to separate elements in CSV file\n",
    " * append:(bool)True:append if file exists(useful for continuing training)\n",
    "                False: overwrite existing file\n",
    "                \n",
    ">>> csvlogger=tf.keras.callbacks.CSVLogger(filename=r\"C:\\Desktop\\file.txt\",separator=',',append=False)\n",
    ">>> history=model.fit(x,steps_per_epoch,epochs,verbose,callbacks=[csvlogger])\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------    \n",
    "2) ProgbarLogger():\n",
    "@ tf.keras.callbacks.ProgbarLogger(count_mode=\"samples\",stateful_metrics=None)\n",
    "  * count_mode: \"samples\":Display No. of samples Seen or \"steps\":No. of batches seen\n",
    "  * stateful_metrics:Iterable of 'metrics' that should not be averaged over epoch(Metrics in list will be logged)\n",
    "        \n",
    ">>> progbar=tf.keras.callbacks.ProgbarLogger(count_mode='samples',stateful_metrics=None)\n",
    ">>> history=model.fit(x,steps_per_epoch,epochs,verbose,callbacks=[progbar])\n",
    "\n",
    "Exception: \n",
    "    valueError: In case of invalid 'count_mode'\n",
    "\n",
    "\n",
    "## EarlyStopping() , LearningRateScheduler(), ReduceLROnPlateau() ,  TerminateOnNaN()\n",
    "\n",
    "**A) EarlyStopping():** Stop training when 'monitored metric has stopped improving'\n",
    "https://keras.io/api/callbacks/early_stopping/\n",
    "\n",
    "\n",
    "**B) LearningRateScheduler :** Changes the value of Learning function as per Function\n",
    "- At beginning of every epoch,this callback gets updated 'learning rate value' from schedule function provided at '__init__'\n",
    "- current epoch & current learning rate, applies updated learning rate on optimizer\n",
    "https://keras.io/api/callbacks/learning_rate_scheduler/            \n",
    "\n",
    "\n",
    "**C)ReduceLROnPlateau() :** Reduces Lrate if loss value does no longer improve.\n",
    "- Reduce learning rate when **Metric** has stopped improving.\n",
    "- Monitors **Metric** & no Improvement seen in **patience** no. of epochs.\n",
    "- Then LRate reduced by **factor** & Improvement specified by **min_delta **,if no Improvement \n",
    "***Note:*** LRate never decrease beyond \"min_lr\"\n",
    "\n",
    "**D)TerminateOnNaN() :**\n",
    "-  If loss value is **NaN**, training process stops\n",
    "\n",
    "\n",
    "\n",
    "A) EarlyStopping():\n",
    "@ tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0,patience=0,verbose=0,mode=\"auto\",baseline=None,restore_best_weights=False)\n",
    " * monitor: Quantity to be monitored(loss-metrics)\n",
    " * min_delta: Min change in monitored quantity to qualify as an improvement(absolute change of less than 'min_delta',count as no improvement)\n",
    " * patience: No. of epochs with no improvement after which training will be stopped\n",
    " * verbose: verbosity mode\n",
    " * mode=\"min\":training stop when quantity monitored has stopped decreasing; \n",
    "       =\"max\": stop when quantity monitored has stopped increasing\n",
    "       =\"auto\":direction is automatically inferred from name of monitored quantity\n",
    " * baseline: Baseline value for monitored quantity\n",
    "            Training stop,if  model does not improv over baseline.\n",
    " * restore_best_weights:Whether to restore model weights from epoch with best value of monitored quantity\n",
    "     False:model weights obtained at last step of training are used\n",
    "\n",
    ">>> callback=tf.keras.callbacks.EarlyStopping(monitor='loss',patience=3)\n",
    ">>> history=model.fit(x,epochs,batch_size,callbacks=[callback],verbose)\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "B) LearningRateScheduler():\n",
    "@ tf.keras.callbacks.LearningRateScheduler(schedule,verbose=0)\n",
    " * schedule:function,takes an epoch index(int,indexed from 0) & current learning rate(float) as inputs & returns new 'learning rate' as output(float)\n",
    " * verbose=0:quiet & 1:update messages\n",
    "\n",
    "def scheduler(epoch,lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "else:\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "\n",
    ">>> rl_scheduler=tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    ">>> history=model.fit(x,epochs,batch_size,callbacks=[rl_scheduler],verbose)\n",
    ">>> model.optimizer.lr #Learning Rate\n",
    "------------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "C) ReduceLROnPlateau()\n",
    "@ ReduceLROnPlateau(monitor=\"val_loss\",factor=0.1,patience=10,verbose=0,mode=\"auto\",min_delta=0.0001,cooldown=0,min_lr=0)\n",
    " * monitor:Quantity to be monitored(score)\n",
    " * factor:Reduced Learning rate factor (new_lr=lr*factor)\n",
    " * patience:No. of epochs with no improvement after which 'LR' will be reduced.\n",
    " * verbose:(int) 0:quiet | 1:update messages.\n",
    " * mode:{'auto'|'min'|'max'}\n",
    "    'min':LRate reduced when quantity monitored has stopped decreasing\n",
    "    'max': Reduced when quantity monitored has stopped increasing\n",
    "    'auto': Automatically inferred from name of monitored quantity\n",
    " * min_delta: Threshold for measuring new optimum,to only focus on significant changes\n",
    " * cooldown:No. of epochs to wait before resuming normal operation after lr has been reduced\n",
    " * min_lr: lower bound on learning rate.\n",
    "\n",
    ">>> red_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=5,min_lr=0.0001)\n",
    ">>> model.fit(X_train,Y_train,callbacks=[red_lr])\n",
    "------------------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "D) TerminateOnNaN()\n",
    "@ tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    ">>> terminater=tf.keras.callbacks.TerminateOnNaN()\n",
    ">>> model.fit(X_train,Y_train,callbacks=[terminater])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## LambdaCallback: \n",
    "- Allows to define simple functions that can be executed as callback\n",
    "\n",
    "\n",
    "- Arguments:\n",
    "    - **on_epoch_begin:** called at ***beginning of every epoch***(parameter: epoch,logs)\n",
    "    - **on_epoch_end:** called at ***end of every epoch***(parameter: epoch,logs)\n",
    "    - **on_batch_begin:** called at ***beginning of every batch***(parameter: epoch,logs)\n",
    "    - **on_batch_end:** called at ***end of every batch***(parameter: epoch,logs)\n",
    "    - **on_train_begin:** called at ***beginning of model training***(parameter: logs)\n",
    "    - **on_train_end:** called at ***end of model training***(parameter: logs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@ tf.keras.callbacks.LambdaCallback(on_epoch_begin=None,on_epoch_end=None,on_batch_begin=None,\n",
    "    on_batch_end=None,on_train_begin=None,on_train_end=None)\n",
    "\n",
    "# Stream the epoch loss to a file in JSON format. The file content\n",
    "# is not well-formed JSON but rather has a JSON object per line.\n",
    "\n",
    "import json\n",
    "json_log = open('loss_log.json', mode='wt', buffering=1)\n",
    "json_logging_callback = LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: json_log.write(\n",
    "        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),\n",
    "    on_train_end=lambda logs: json_log.close()\n",
    ")\n",
    "\n",
    "# Terminate some processes after having finished model training.\n",
    "processes = ...\n",
    "cleanup_callback = LambdaCallback(\n",
    "    on_train_end=lambda logs: [\n",
    "        p.terminate() for p in processes if p.is_alive()])\n",
    "\n",
    "model.fit(...,\n",
    "          callbacks=[batch_print_callback,\n",
    "                     json_logging_callback,\n",
    "                     cleanup_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
